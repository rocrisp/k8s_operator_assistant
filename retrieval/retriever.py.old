#from langchain.embeddings import SentenceTransformerEmbeddings
from langchain_ibm import WatsonxEmbeddings
from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams
from ibm_watsonx_ai import Credentials
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
import os

class Retriever:
    def __init__(self, path):
        embed_params = {
            EmbedParams.TRUNCATE_INPUT_TOKENS: 128,
            EmbedParams.RETURN_OPTIONS: {'input_text': True},
        }
        # Check if the API key is set as an environment variable
        api_key = os.getenv('WATSONX_APIKEY')
        project_key = os.getenv("WATSONX_PROJECTKEY")
        
        credentials = Credentials(
            url="https://us-south.ml.cloud.ibm.com",
            api_key=api_key,
        )
        
        self.documents = self._load_documents_from_text(path)
        self.splits = self._split_docs(self.documents)
        self.embeddings = WatsonxEmbeddings(
            model_id="ibm/slate-125m-english-rtrvr",
            url=credentials.get("url"),
            params=embed_params,
            apikey=credentials.get("api_key"),
            project_id=project_key
        )
        self.docsearch = Chroma.from_documents(self.splits, self.embeddings)
        print(self.docsearch)
        #self.vectorstore = self._populate_vectorstore(self.splits)
        
        #self.retriever = self.vectorstore.as_retriever()

    def _load_documents_from_text(self, path):
        # Initialize an empty list to store all text chunks
        all_texts = []
        
        # Process each text file in the directory
        for filename in os.listdir(path):
            if filename.endswith(".txt"):
                # Load the text file
                print(f"Processing {filename}")
                filepath = os.path.join(path, filename)
                loader = TextLoader(filepath)
                documents = loader.load()

                # Split the text into chunks
                text_splitter = CharacterTextSplitter(chunk_size=1300, chunk_overlap=0)
                texts = text_splitter.split_documents(documents)
                all_texts.extend(texts)
        return all_texts

    def _split_docs(self, data):
        #text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
        text_splitter = CharacterTextSplitter(chunk_size=1300, chunk_overlap=0)
        texts = text_splitter.split_documents(data)
        return texts

    def get_docsearch(self):
        return self.docsearch 
    

    # def retrieve(self, query, k=5):
    #     results = self.vectorstore.similarity_search(query, k=k)
    #     return results