{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I am Open-Assistant, a chatbot based on a large language model. You can ask '\n",
      " 'me anything, I will do my best to provide you helpful answers.')\n",
      "('I can help you write a Golang function to reconcile the version of a '\n",
      " 'Kubernetes Deployment for a container App.')\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "ollama = Ollama(base_url=\"http://localhost:11434\", model=\"autopilot\")\n",
    "\n",
    "TEXT_PROMPT1 = \"who are you?\"\n",
    "TEXT_PROMPT2 = \"what can you do?\"\n",
    "from pprint import pprint\n",
    "pprint(ollama(TEXT_PROMPT1))\n",
    "pprint(ollama(TEXT_PROMPT2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To develop a Kubernetes operator, you can use the Operator SDK. Here is a step-by-step guide on how to do this:\n",
      "\n",
      "1. Install Operator SDK: Follow the installation instructions in the Operator SDK documentation.\n",
      "2. Create a new project: Use the following command to create a new project for your operator:\n",
      "\n",
      "```bash\n",
      "$ cd ${HOME}\n",
      "$ mkdir sample-operator && cd $_\n",
      "$ operator-sdk init --domain sample.com --owner \"The Sample Operator Team <sample-operator@email.com>\"\n",
      "$ go mod vendor\n",
      "```\n",
      "\n",
      "3. Add custom resource definition (CRD): Create a new file called `bases/rose.yaml` and add the following content:\n",
      "\n",
      "```yaml\n",
      "apiVersion: apiextensions.k8s.io/v1beta1\n",
      "kind: CustomResourceDefinition\n",
      "metadata:\n",
      "  name: roses.samples.com\n",
      "spec:\n",
      "  group: samples.com\n",
      "  names:\n",
      "    kind: Rose\n",
      "    listKind: RoseList\n",
      "    plural: roses\n",
      "    singular: rose\n",
      "  scope: Namespaced\n",
      "  version: v1alpha1\n",
      "```\n",
      "\n",
      "4. Implement controller logic: Create a new file called `controllers/rose_controller.go` and add the following content:\n",
      "\n",
      "```go\n",
      "package controllers\n",
      "\n",
      "import (\n",
      "    \"context\"\n",
      "\n",
      "    samplesv1alpha1 \"github.com/your-username/sample-operator/pkg/apis/samples/v1alpha1\"\n",
      "    \"sigs.k8s.io/controller-runtime/pkg/client\"\n",
      "    \"sigs.k8s.io/controller-runtime/pkg/controller\"\n",
      "    \"sigs.k8s.io/controller-runtime/pkg/event\"\n",
      "    \"sigs.k8s.io/controller-runtime/pkg/handler\"\n",
      "    \"sigs.k8s.io/controller-runtime/pkg/manager\"\n",
      "    \"sigs.k8s.io/controller-runtime/pkg/predicate\"\n",
      "    \"sigs.k8s.io/controller-runtime/pkg/source\"\n",
      ")\n",
      "\n",
      "type RoseReconciler struct {\n",
      "    client.Client\n",
      "    Log    logr.Logger\n",
      "   Scheme *runtime.Scheme\n",
      "}\n",
      "\n",
      "func (r *RoseReconciler) Reconcile(request reconcile.Request) (reconcile.Result, error) {\n",
      "    // Implement your logic here to reconcile the custom resource\n",
      "\n",
      "    return reconcile.Result{}, nil\n",
      "}\n",
      "```\n",
      "\n",
      "5. Build and deploy the operator: Use the following commands to build and deploy the operator:\n",
      "\n",
      "```bash\n",
      "$ make docker-build docker-push IMG=<your-dockerhub-username>/sample-operator:v0.0.1\n",
      "$ kubectl apply -f config/crds\n",
      "$ kubectl create namespace sample-operator\n",
      "$ kubectl label namespace sample-operator operand-registry=true\n",
      "$ kubectl apply -f config/rbac/\n",
      "$ kubectl apply -f config/manager/\n",
      "```\n",
      "\n",
      "6. Test the operator: Create a new instance of the custom resource using the following command:\n",
      "\n",
      "```bash\n",
      "$ cat <<EOF | kubectl apply -f -\n",
      "apiVersion: samples.com/v1alpha1\n",
      "kind: Rose\n",
      "metadata:\n",
      "  name: sample-rose\n",
      "  namespace: default\n",
      "spec:\n",
      "  # Add any specific configuration for your custom resource here\n",
      "EOF\n",
      "```\n",
      "\n",
      "7. Verify the operator's behavior: Check if the operator deployed and reconciled the custom resource as expected. You can also view the logs of the running pods to see the operator's behavior.\n",
      "\n",
      "That's it! You have successfully developed and deployed a Kubernetes operator using Operator SDK, with the ability to deploy your own containerized application through the custom resource \"Rose\".\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the template with placeholders\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "You are an expert in Kubernetes development. You will use Operator SDK as the guide to aid in the development of a Kubernetes operator. The operator will be used to deploy a containerized application. The application will be deployed using the following details:\n",
    "\n",
    "The operator will deploy custom resource: {resource}\n",
    "\n",
    "with the pod image: {image}\n",
    "\n",
    "Output step by step instruction on how to develope and deploy Kubernetes operator using Operator SDK.:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"resource\", \"image\"]) \n",
    "\n",
    "# Example article details\n",
    "\n",
    "custom_resource = \"rose\"\n",
    "\n",
    "operand_image = \"\"\"\n",
    "\n",
    "nginx:latest\n",
    "\"\"\"\n",
    "\n",
    "# Fill the template with the article details\n",
    "\n",
    "prompt = prompt_template.format(resource=custom_resource, image=operand_image)\n",
    "\n",
    "# Get the summary from the language model\n",
    "\n",
    "def chat_with(llm, prompt):\n",
    "    return llm.invoke(prompt) \n",
    "\n",
    "ollama = Ollama(base_url=\"http://localhost:11434\", model=\"autopilot\")\n",
    "\n",
    "summary = chat_with(ollama,prompt)    \n",
    "\n",
    "print(summary)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Define a prompt template\n",
    "chain1_prompt_template = PromptTemplate(template=\"{text}\", input_variables=[\"text\"]) \n",
    "\n",
    "chain2__prompt = PromptTemplate(\n",
    "    input_variables=[\"joke\"], \n",
    "    template=\"First, Explain the following joke: {joke}, Second,then tell me a better joke.\"\n",
    ")\n",
    "\n",
    "chain3_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"joke1\", \"joke2\"],\n",
    "    template=\"Can you tell me if joke1: {joke1}, is better than joke2: {joke2}\"\n",
    ")      \n",
    "\n",
    "# Define the chain\n",
    "chain1 = LLMChain(\n",
    "    llm=ollama,\n",
    "    prompt=prompt_template,\n",
    "    output_key=\"joke1\"\n",
    ")\n",
    "chain2 = LLMChain(\n",
    "    llm=ollama,\n",
    "    prompt=prompt_template,\n",
    "    output_key=\"joke2\"\n",
    ")\n",
    "chain3 = LLMChain(\n",
    "    llm=ollama,\n",
    "    prompt=chain3_prompt_template,\n",
    "    output_key=\"critique\"\n",
    ") \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains=[\n",
    "        chain1,\n",
    "        chain2,\n",
    "        chain3\n",
    "        \n",
    "    ],\n",
    "    input_variables=[\"text\"],\n",
    "    output_variables = [\"joke1\",\"joke2\", \"critique\"],\n",
    "    verbose=True\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'critique': 'Both are jokes, but I would say that joke1 is more surprising '\n",
      "             'and unexpected. In joke1, the element of surprise comes from the '\n",
      "             'unusual pairing of an elephant and a blue whale, while in joke2, '\n",
      "             'the surprise comes from the chicken actually crossing the '\n",
      "             'playground.',\n",
      " 'joke1': \"Here's one, what do you get when you cross an elephant and a blue \"\n",
      "          'whale?',\n",
      " 'joke2': 'Why did the chicken cross the playground? To get to the other '\n",
      "          'slide.',\n",
      " 'text': 'tell me a joke'}\n"
     ]
    }
   ],
   "source": [
    "result = chain({'text': 'tell me a joke'})\n",
    "pprint(result)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.nvidia.com', 'title': 'NVIDIA Documentation Hub - NVIDIA Docs', 'description': 'Get started by exploring the latest technical information and product documentation', 'language': 'en'}, page_content=\"NVIDIA Documentation Hub - NVIDIA DocsSubmit SearchNVIDIA DeveloperBlogForumsJoinSubmit SearchNVIDIA DeveloperBlogForumsJoinMenuNVIDIA Documentation HubGet started by exploring the latest technical information and product documentationBrowse byFeaturedProductsAll DocumentsSubmit SearchMost PopularGet support for our latest innovations and see how you can bring them into your own work.NVIDIA API Documentation        Your guide to NVIDIA APIs including NIM and CUDA-X microservices.                Browse            NVIDIA AI Enterprise        NVIDIA AI Enterprise is an end-to-end, cloud-native software platform that accelerates data science pipelines and streamlines development and deployment of production-grade co-pilots and other generative AI applications.                 Browse            NVIDIA Omniverse        NVIDIA Omniverse is a cloud-native, multi-GPU, real-time simulation and collaboration platform for 3D production pipelines based on Pixar's Universal Scene Description (USD) and NVIDIA RTX.                Browse            NVIDIA CUDA        The NVIDIA® CUDA® Toolkit provides a comprehensive development environment for C and C++ developers building GPU-accelerated applications.                Browse            NVIDIA DGX Platform        Built from the ground up for enterprise AI, the NVIDIA DGX platform incorporates the best of NVIDIA software, infrastructure, and expertise in a modern, unified AI development and training solution.                 Browse            NVIDIA cuDNN        The NVIDIA CUDA® Deep Neural Network (cuDNN) is a GPU-accelerated library of primitives for deep neural networks.                 Browse            NVIDIA Jetson        The NVIDIA JetPack SDK, which is the most comprehensive solution for building AI applications, along with L4T and L4T Multimedia, provides the Linux kernel, bootloader, NVIDIA drivers, flashing utilities, sample filesystem, and more for the Jetson platform.                Browse            NVIDIA TensorRT        NVIDIA TensorRT is an SDK for high-performance deep learning inference. It is designed to work in a complementary fashion with training frameworks such as TensorFlow, PyTorch, and MXNet. It focuses specifically on running an already-trained network quickly and efficiently on NVIDIA hardware.                Browse            OpenFiltersClose filtersFiltersSelected FiltersClear AllTopics            AR / VR(4)            Computer Vision / Video Analytics(7)            Content Creation / Rendering(9)            Conversational AI(2)            Cybersecurity(2)            Data Center / Cloud(30)            Data Science(9)            Edge Computing(11)            Generative AI / LLMs(2)            Networking(3)            Recommenders / Personalization(3)            Robotics(4)            Simulation / Modeling / Design(11)See AllSee LessIndustry Segments            Academia / Higher Education(37)            Aerospace(42)            Agriculture(38)            Architecture / Engineering / Construction(41)            Automotive / Transportation(9)            Cloud Services(41)            Consumer Internet(37)            Energy(42)            Financial Services(40)            Gaming(48)            Hardware / Semiconductor(40)            Healthcare & Life Sciences(22)            HPC / Scientific Computing(48)            Manufacturing(40)            Media & Entertainment(44)            Public Sector(42)            Restaurant / Quick-Service(39)            Retail / Consumer Packaged Goods(40)            Telecommunications(39)See AllSee LessJob Roles            Artist / Designer(3)            Business Executive(3)            Data Scientist(23)            Dev / IT Operations(38)            Developer / Engineer(60)            Research: Academic(6)            Research: Non-Academic(6)See AllSee LessAPPLYSort by:Alphabetical - A-ZNewestDocumentation CenterCloudera Data Platform (CDP)04/10/23        The integration of NVIDIA RAPIDS into the Cloudera Data Platform (CDP) provides transparent GPU acceleration of data analytics workloads using Apache Spark. This documentation describes the integration and suggested reference architectures for deployment.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterFleet Command User Guide06/11/24        NVIDIA Fleet Command brings secure edge AI to enterprises of any size. Transform NVIDIA-certified servers into secure edge appliances and connect them to the cloud in minutes. From the cloud, deploy and manage applications from the NGC Catalog or your NGC Private Registry, update system software over-the-air and manage systems remotely with nothing but a browser and internet connection.    Cloud ServicesEdge ComputingData Center / CloudDocumentation CenterGPU Management and Deployment01/23/23        This documentation should be of interest to cluster admins and support personnel of enterprise GPU deployments. It includes monitoring and management tools and application programming interfaces (APIs), in-field diagnostics and health monitoring, and cluster setup and deployment.    Documentation CenterNVIDIA Megatron-Core03/16/24        Developer documentation for Megatron Core covers API documentation, quickstart guide as well as deep dives into advanced GPU  techniques needed to optimize LLM performance at scale.    Documentation CenternvCOMP01/23/23        nvCOMP is a high performance GPU enabled data compression library. Includes both open-source and non-OS components. The nvCOMP library provides fast lossless data compression and decompression using a GPU. It features generic compression interfaces to enable developers to use high-performance GPU compressors in their applications.    ProductNVIDIA Aerial04/12/23NVIDIA Aerial™ is a suite of accelerated computing platforms, software, and services for designing, simulating, and operating wireless networks. Aerial contains hardened RAN software libraries for telcos, cloud service providers (CSPs), and enterprises building commercial 5G networks. Academic and industry researchers can access Aerial on cloud or on-premises setups for advanced wireless and AI/machine learning (ML) research for 6G.    Edge ComputingTelecommunicationsProductNVIDIA AI Enterprise04/27/23        NVIDIA AI Enterprise is an end-to-end, cloud-native software platform that accelerates data science pipelines and streamlines development and deployment of production-grade co-pilots and other generative AI applications. Easy-to-use microservices provide optimized model performance with enterprise-grade security, support, and stability to ensure a smooth transition from prototype to production for enterprises that run their businesses on AI.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA Air06/12/23        A simulation platform that allows users to model data center deployments with full software functionality, creating a digital twin. Transform and streamline network operations by simulating, validating, and automating changes and updates.    Documentation CenterNVIDIA Ansel02/03/23        NVIDIA Ansel is a revolutionary way to capture in-game shots and share the moment. Compose your screenshots from any position, adjust them with post-process filters, capture HDR images in high-fidelity formats, and share them in 360 degrees using your mobile phone, PC, or VR headset.    Documentation CenterNVIDIA API Documentation05/09/24        Your guide to NVIDIA APIs including NIM and CUDA-X microservices.    ProductNVIDIA Base Command Manager10/30/23        NVIDIA Base Command Manager streamlines cluster provisioning, workload management, and infrastructure monitoring. It provides all the tools you need to deploy and manage an AI data center. NVIDIA Base Command Manager Essentials comprises the features of NVIDIA Base Command Manager that are certified for use with NVIDIA AI Enterprise.    Data Center / CloudTechnical OverviewNVIDIA Base Command Platform01/25/23        NVIDIA Base Command Platform is a world-class infrastructure solution for businesses and their data scientists who need a premium AI development experience.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceProductNVIDIA Base OS04/18/23        NVIDIA Base OS implements the stable and fully qualified operating systems for running AI, machine learning, and analytics applications on the DGX platform. It includes system-specific configurations, drivers, and diagnostic and monitoring tools and is available for Ubuntu, Red Hat Enterprise Linux, and Rocky Linux.    Data Center / CloudDocumentation CenterNVIDIA Bright Cluster Manager 03/08/23        NVIDIA Bright Cluster Manager offers fast deployment and end-to-end management for heterogeneous HPC and AI server clusters at the edge, in the data center and in multi/hybrid-cloud environments. It automates provisioning and administration for clusters ranging in size from a single node to hundreds of thousands, supports CPU-based and NVIDIA GPU-accelerated systems, and orchestration with Kubernetes.    HPC / Scientific ComputingEdge ComputingData Center / CloudDocumentation CenterNVIDIA Capture SDK01/23/23        NVIDIA Capture SDK (formerly GRID SDK) enables developers to easily and efficiently capture, and optionally encode, the display content.    Documentation CenterNVIDIA Certification Programs02/06/23        NVIDIA’s program that enables enterprises to confidently deploy hardware solutions that optimally run accelerated workloads—from desktop to data center to edge.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceProductNVIDIA Clara04/27/23        NVIDIA® Clara™ is an open, scalable computing platform that enables developers to build and deploy medical imaging applications into hybrid (embedded, on-premises, or cloud) computing environments to create intelligent instruments and automate healthcare workflows.    Healthcare & Life SciencesComputer Vision / Video AnalyticsProductNVIDIA Cloud Functions        Serverless API to deploy and manage AI workloads on GPUs at planetary scale.    ProductNVIDIA Cloud Native Technologies01/23/23        NVIDIA cloud-native technologies enable developers to build and run GPU-accelerated containers using Docker and Kubernetes.    Cloud ServicesData Center / CloudDocumentation CenterNVIDIA CloudXR SDK02/27/23        CloudXR is NVIDIA's solution for streaming virtual reality (VR), augmented reality (AR), and mixed reality (MR) content from any OpenVR XR application on a remote server--desktop, cloud, data center, or edge.    Documentation CenterNVIDIA Compute Sanitizer04/25/23        Compute Sanitizer is a functional correctness checking suite included in the CUDA toolkit. This suite contains multiple tools that can perform different type of checks. The memcheck tool is capable of precisely detecting and attributing out of bounds and misaligned memory access errors in CUDA applications. The tool can also report hardware exceptions encountered by the GPU. The racecheck tool can report shared memory data access hazards that can cause data races. The initcheck tool can report cases where the GPU performs uninitialized accesses to global memory. The synccheck tool can report cases where the application is attempting invalid usages of synchronization primitives. This document describes the usage of these tools.    ProductNVIDIA CUDA04/03/23        The NVIDIA® CUDA® Toolkit provides a comprehensive development environment for C and C++ developers building GPU-accelerated applications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA cuDNN04/12/23        The NVIDIA CUDA® Deep Neural Network (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, attention, matmul, pooling, and normalization.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceProductNVIDIA cuOpt07/14/23        NVIDIA cuOpt™ is a GPU-accelerated solver that uses heuristics and metaheuristics to solve complex vehicle routing problem variants with a wide range of constraints.    Data ScienceRoboticsProductNVIDIA DALI03/22/23        The NVIDIA Data Loading Library (DALI) is a collection of highly optimized building blocks, and an execution engine, for accelerating the pre-processing of input data for deep learning applications. DALI provides both the performance and the flexibility for accelerating different data pipelines as a single library. This single library can then be easily integrated into different deep learning training and inference applications.    AerospaceHardware / SemiconductorArchitecture / Engineering / ConstructionDocumentation CenterNVIDIA Data Center GPU Drivers01/23/23        NVIDIA Data Center GPU drivers are used in Data Center GPU enterprise deployments for AI, HPC, and accelerated computing workloads. Documentation includes release notes, supported platforms, and cluster setup and deployment.    Documentation CenterNVIDIA Data Center GPU Manager (DCGM)02/03/23        NVIDIA Data Center GPU Manager (DCGM) is a suite of tools for managing and monitoring NVIDIA Data Center GPUs in cluster environments.    AerospaceHardware / SemiconductorArchitecture / Engineering / ConstructionDocumentation CenterNVIDIA Deep Graph Library (DGL)01/23/23        Deep Graph Library (DGL) is a framework-neutral, easy-to-use, and scalable Python library used for implementing and training Graph Neural Networks (GNN). Being framework-neutral, DGL is easily integrated into an existing PyTorch, TensorFlow, or an Apache MXNet workflow.    Documentation CenterNVIDIA Deep Learning Performance07/27/23        GPUs accelerate machine learning operations by performing calculations in parallel. Many operations, especially those representable as matrix multipliers will see good acceleration right out of the box. Even better performance can be achieved by tweaking operation parameters to efficiently use GPU resources. The performance documents present the tips that we think are most widely useful.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceProductNVIDIA DGX Cloud04/26/24        NVIDIA DGX Cloud is an AI platform for enterprise developers, optimized for the demands of generative AI.    ProductNVIDIA DGX Platform11/03/23        Built from the ground up for enterprise AI, the NVIDIA DGX platform incorporates the best of NVIDIA software, infrastructure, and expertise in a modern, unified AI development and training solution. Every aspect of the DGX platform is infused with NVIDIA AI expertise, featuring world-class software, record-breaking NVIDIA-accelerated infrastructure in the cloud or on-premises, and direct access to NVIDIA DGXPerts to speed the ROI of AI for every enterprise.    Hardware / SemiconductorArchitecture / Engineering / ConstructionHPC / Scientific ComputingProductNVIDIA DGX SuperPOD03/17/23        Deployment and management guides for NVIDIA DGX SuperPOD, an AI data center infrastructure platform that enables IT to deliver performance—without compromise—for every user and workload. DGX SuperPOD offers leadership-class accelerated infrastructure and agile, scalable performance for the most challenging AI and high-performance computing (HPC) workloads, with industry-proven results.    Data Center / CloudProductNVIDIA DGX Systems04/24/23        System documentation for the DGX AI supercomputers that deliver world-class performance for large generative AI and mainstream AI workloads.    Data Center / CloudDocumentation CenterNVIDIA DIGITS02/03/23        The NVIDIA Deep Learning GPU Training System (DIGITS) can be used to rapidly train highly accurate deep neural networks (DNNs) for image classification, segmentation, and object-detection tasks. DIGITS simplifies common deep learning tasks such as managing data, designing and training neural networks on multi-GPU systems, monitoring performance in real time with advanced visualizations, and selecting the best-performing model from the results browser for deployment.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA DRIVE01/23/23        Learn how to develop for NVIDIA DRIVE®, a scalable computing platform that enables automakers and Tier-1 suppliers to accelerate production of autonomous vehicles.    Documentation CenterNVIDIA EGX01/23/23        The NVIDIA EGX platform delivers the power of accelerated AI computing to the edge with a cloud-native software stack (EGX stack), a range of validated servers and devices, Helm charts, and partners who offer EGX through their products and services.    ProductNVIDIA Enterprise Support and Services06/26/23        NVIDIA’s accelerated computing, visualization, and networking solutions are expediting the speed of business outcomes. NVIDIA’s experts are here for you at every step in this fast-paced journey. With our expansive support tiers, fast implementations, robust professional services, market-leading education, and high caliber technical certifications, we are here to help you achieve success with all parts of NVIDIA’s accelerated computing, visualization, and networking platform.    Documentation CenterNVIDIA FLARE (Federated Learning Active Runtime Environment)02/03/23        FLARE (Federated Learning Active Runtime Environment) is Nvidia’s open source extensible SDK that allows researchers and data scientists to adapt existing ML/DL workflow to a privacy preserving federated paradigm. FLARE makes it possible to build robust, generalizable AI models without sharing data.    ProductNVIDIA Gameworks01/25/23        Documentation for GameWorks-related products and technologies, including libraries (NVAPI, OpenAutomate), code samples (DirectX, OpenGL), and developer tools (Nsight, NVIDIA System Profiler).    GamingContent Creation / RenderingDocumentation CenterNVIDIA GeForce NOW Developer Platform02/03/23        The GeForce NOW Developer Platform is an SDK and toolset empowering integration of, interaction with, and testing on the NVIDIA cloud gaming service.    Documentation CenterNVIDIA GPUDirect Storage06/14/24        NVIDIA GPUDirect Storage (GDS) enables the fastest data path between GPU memory and storage by avoiding copies to and from system memory, thereby increasing storage input/output (IO) bandwidth and decreasing latency and CPU utilization.    AerospaceHardware / SemiconductorArchitecture / Engineering / ConstructionProductNVIDIA Grace05/30/24        Grace is NVIDIA’s first datacenter CPU. Comprising 72 high-performance Arm v9 cores and featuring the NVIDIA-proprietary Scalable Coherency Fabric (SCF) network-on-chip for incredible core-to-core communication, memory bandwidth and GPU I/O capabilities, Grace provides a high-performance compute foundation in a low-power system-on-chip.    Data Center / CloudDocumentation CenterNVIDIA GVDB Voxels02/03/23        NVIDIA GVDB Voxels is a new framework for simulation, compute and rendering of sparse voxels on the GPU.    Documentation CenterNVIDIA Highlights02/03/23        NVIDIA Highlights enables automatic video capture of key moments, clutch kills, and match-winning plays, ensuring gamers’ best gaming moments are always saved. Once a Highlight is captured, gamers can simply share it directly to Facebook, YouTube, or Weibo right from GeForce Experience’s in-game overlay. Additionally, they can also clip their favorite 15 seconds and share as an animated GIF - all without leaving the game!    ProductNVIDIA Holoscan07/25/23        NVIDIA Holoscan is a hybrid computing platform for medical devices that combines hardware systems for low-latency sensor and network connectivity, optimized libraries for data processing and AI, and core microservices to run surgical video, ultrasound, medical imaging, and other applications anywhere, from embedded to edge to cloud.    Healthcare & Life SciencesDocumentation CenterNVIDIA HPC SDK01/23/23        The NVIDIA HPC SDK is a comprehensive suite of compilers, libraries, and development tools used for developing HPC applications for the NVIDIA platform.    ProductNVIDIA IGX Orin03/23/23        NVIDIA IGX Orin™ is an industrial-grade platform that combines enterprise-level hardware, software, and support. As a single, holistic platform, IGX allows companies to focus on application development and realize the benefits of AI faster.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA IndeX02/03/23        NVIDIA IndeX is a 3D volumetric interactive visualization SDK that allows scientists and researchers to visualize and interact with massive data sets, make real-time modifications, and navigate to the most pertinent parts of the data, all in real-time, to gather better insights faster. IndeX leverages GPU clusters for scalable, real-time, visualization and computing of multi-valued volumetric data together with embedded geometry data.            Load More    OpenFiltersClose filtersFiltersSelected FiltersClear AllProducts            Deep Learning Performance(1)            NVIDIA Air(1)            NVIDIA Virtual GPU (vGPU)(1)Topics            AR / VR(4)            Computer Vision / Video Analytics(7)            Content Creation / Rendering(9)            Conversational AI(2)            Cybersecurity(2)            Data Center / Cloud(30)            Data Science(9)            Edge Computing(11)            Generative AI / LLMs(2)            Networking(3)            Recommenders / Personalization(3)            Robotics(4)            Simulation / Modeling / Design(11)See AllSee LessIndustry Segments            Academia / Higher Education(37)            Aerospace(42)            Agriculture(38)            Architecture / Engineering / Construction(41)            Automotive / Transportation(9)            Cloud Services(41)            Consumer Internet(37)            Energy(42)            Financial Services(40)            Gaming(48)            Hardware / Semiconductor(40)            Healthcare & Life Sciences(22)            HPC / Scientific Computing(48)            Manufacturing(40)            Media & Entertainment(44)            Public Sector(42)            Restaurant / Quick-Service(39)            Retail / Consumer Packaged Goods(40)            Telecommunications(39)See AllSee LessContent Types            Documentation Center(9)            Product(1)            Technical Guide(1)Job Roles            Artist / Designer(3)            Business Executive(3)            Data Scientist(23)            Dev / IT Operations(38)            Developer / Engineer(60)            Research: Academic(6)            Research: Non-Academic(6)See AllSee LessJob Titles            CEO(2)            CTO(5)            Data Analyst(4)            Data Scientist(9)            Developer / Engineer(56)            Film / Video Editor(2)            Graphic Designer / Animator(2)            Industrial Designer / Product Designer(2)            IT Specialist(15)            Solutions Architect(1)            System Administrator(10)See AllSee LessAPPLYProductNVIDIA Virtual GPU (vGPU) Software01/23/23        NVIDIA virtual GPU (vGPU) software is a graphics virtualization platform that extends the power of NVIDIA GPU technology to virtual desktops and apps, offering improved security, productivity, and cost-efficiency.    ProductNVIDIA Clara04/27/23        NVIDIA® Clara™ is an open, scalable computing platform that enables developers to build and deploy medical imaging applications into hybrid (embedded, on-premises, or cloud) computing environments to create intelligent instruments and automate healthcare workflows.    Healthcare & Life SciencesComputer Vision / Video AnalyticsDocumentation CenterNVIDIA Triton Inference Server04/17/23        NVIDIA Triton Inference Server (formerly TensorRT Inference Server) provides a cloud inferencing solution optimized for NVIDIA GPUs. The server provides an inference service via an HTTP or gRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server.    Documentation CenterNVIDIA Maxine02/03/23        NVIDIA Maxine is a GPU-accelerated SDK with state-of-the-art AI features for developers to build virtual collaboration and content creation applications such as video conferencing and live streaming. Maxine’s AI SDKs, such as Video Effects, Audio Effects, and Augmented Reality (AR) are highly optimized and include modular features that can be chained into end-to-end pipelines to deliver the highest performance possible on GPUs, both on PCs and in data centers.    AerospaceHardware / SemiconductorArchitecture / Engineering / ConstructionProductNVIDIA LaunchPad04/26/23        NVIDIA LaunchPad is a free program that provides users short-term access to a large catalog of hands-on labs. Now enterprises and organizations can immediately tap into the necessary hardware and software stacks to experience end-to-end solution workflows in the areas of AI, data science, 3D design collaboration and simulation, and more.    Edge ComputingData Center / CloudDocumentation CenterNVIDIA GeForce NOW Developer Platform02/03/23        The GeForce NOW Developer Platform is an SDK and toolset empowering integration of, interaction with, and testing on the NVIDIA cloud gaming service.    Documentation CenterNVIDIA VRWorks Graphics02/03/23        VRWorks™ is a comprehensive suite of APIs, libraries, and engines that enable application and headset developers to create amazing virtual reality experiences. VRWorks enables a new level of presence by bringing physically realistic visuals, sound, touch interactions, and simulated environments to virtual reality.    Documentation CenterNVIDIA Data Center GPU Manager (DCGM)02/03/23        NVIDIA Data Center GPU Manager (DCGM) is a suite of tools for managing and monitoring NVIDIA Data Center GPUs in cluster environments.    AerospaceHardware / SemiconductorArchitecture / Engineering / ConstructionProductNVIDIA Optimized Frameworks        Deep learning (DL) frameworks offer building blocks for designing, training, and validating deep neural networks through a high-level programming interface. Widely-used DL frameworks, such as PyTorch, TensorFlow, PyTorch Geometric, DGL, and others, rely on GPU-accelerated libraries, such as cuDNN, NCCL, and DALI to deliver high-performance, multi-GPU-accelerated training.    Documentation CenterNVIDIA GVDB Voxels02/03/23        NVIDIA GVDB Voxels is a new framework for simulation, compute and rendering of sparse voxels on the GPU.    ProductNVIDIA AI Enterprise04/27/23        NVIDIA AI Enterprise is an end-to-end, cloud-native software platform that accelerates data science pipelines and streamlines development and deployment of production-grade co-pilots and other generative AI applications. Easy-to-use microservices provide optimized model performance with enterprise-grade security, support, and stability to ensure a smooth transition from prototype to production for enterprises that run their businesses on AI.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA MAGNUM IO06/27/23        NVIDIA MAGNUM IO™ software development kit (SDK) enables developers to remove input/output (IO) bottlenecks in AI, high performance computing (HPC), data science, and visualization applications, reducing the end-to-end time of their workflows. Magnum IO covers all aspects of data movement between CPUs, GPUsns, DPUs, and storage subsystems in virtualized, containerized, and bare-metal environments.    ProductNVIDIA CUDA04/03/23        The NVIDIA® CUDA® Toolkit provides a comprehensive development environment for C and C++ developers building GPU-accelerated applications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceProductNVIDIA Trusted Computing Solutions07/25/23        Protecting sensitive and proprietary information using strong hardware-based security.    Healthcare & Life SciencesFinancial ServicesData Center / CloudDocumentation CenterCloudera Data Platform (CDP)04/10/23        The integration of NVIDIA RAPIDS into the Cloudera Data Platform (CDP) provides transparent GPU acceleration of data analytics workloads using Apache Spark. This documentation describes the integration and suggested reference architectures for deployment.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA Metropolis Microservices for Jetson01/11/24        Metropolis Microservices for Jetson is a platform that simplifies development, deployment and management of Edge AI applications on NVIDIA Jetson. It provides a modular & extensible architecture for developers to distill large complex applications into smaller modular microservice with APIs to integrate into other apps & services.    ProductNVIDIA TAO03/27/23        The NVIDIA TAO Toolkit eliminates the time-consuming process of building and fine-tuning DNNs from scratch for IVA applications.    Public SectorEdge ComputingComputer Vision / Video AnalyticsDocumentation CenterNVIDIA SDK Manager02/03/23        NVIDIA SDK Manager is an all-in-one tool that bundles developer software and provides an end-to-end development environment setup solution for NVIDIA SDKs. Learn about the prerequisite hardware and software to get started with NVIDIA SDK Manager. See the latest features and updates.    Documentation CenterNVIDIA Certification Programs02/06/23        NVIDIA’s program that enables enterprises to confidently deploy hardware solutions that optimally run accelerated workloads—from desktop to data center to edge.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA Ansel02/03/23        NVIDIA Ansel is a revolutionary way to capture in-game shots and share the moment. Compose your screenshots from any position, adjust them with post-process filters, capture HDR images in high-fidelity formats, and share them in 360 degrees using your mobile phone, PC, or VR headset.    ProductNVIDIA Holoscan07/25/23        NVIDIA Holoscan is a hybrid computing platform for medical devices that combines hardware systems for low-latency sensor and network connectivity, optimized libraries for data processing and AI, and core microservices to run surgical video, ultrasound, medical imaging, and other applications anywhere, from embedded to edge to cloud.    Healthcare & Life SciencesProductRAPIDS Accelerator for Apache Spark09/14/23        The RAPIDS Accelerator for Apache Spark leverages GPUs to accelerate processing by combining the power of the RAPIDS cuDF library and the scale of the Spark distributed computing framework. You can run your existing Apache Spark applications on GPUs with no code change by launching Spark with the RAPIDS Accelerator for Apache Spark plugin jar and enabling a single configuration setting.    Data ScienceProductNVIDIA License System02/16/23        NVIDIA® License System is used to serve a pool of floating licenses to NVIDIA licensed products. The NVIDIA License System is configured with licenses obtained from the NVIDIA Licensing Portal.    Data Center / CloudProductNVIDIA NIM05/31/24        Part of NVIDIA AI Enterprise, NVIDIA NIM is a set of easy-to-use microservices for accelerating the deployment of foundation models on any cloud or data center and helps keep your data secure. NIM has production-grade runtimes including on-going security updates. Run your business applications with stable APIs backed by enterprise-grade support.    Documentation CenterNVIDIA FLARE (Federated Learning Active Runtime Environment)02/03/23        FLARE (Federated Learning Active Runtime Environment) is Nvidia’s open source extensible SDK that allows researchers and data scientists to adapt existing ML/DL workflow to a privacy preserving federated paradigm. FLARE makes it possible to build robust, generalizable AI models without sharing data.    ProductNVIDIA Base OS04/18/23        NVIDIA Base OS implements the stable and fully qualified operating systems for running AI, machine learning, and analytics applications on the DGX platform. It includes system-specific configurations, drivers, and diagnostic and monitoring tools and is available for Ubuntu, Red Hat Enterprise Linux, and Rocky Linux.    Data Center / CloudTechnical OverviewNVIDIA Base Command Platform01/25/23        NVIDIA Base Command Platform is a world-class infrastructure solution for businesses and their data scientists who need a premium AI development experience.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA NVTAGS03/22/23        NVIDIA Topology-Aware GPU Selection (NVTAGS) intelligently and automatically assigns GPUs to MPI processes, which reduces overall GPU-to-GPU communication time for Message Passing Interface (MPI) applications.    HPC / Scientific ComputingData Center / CloudDocumentation CenterNVIDIA Rivermax02/03/23        Unique IP-based solution that boosts video and data streaming performance. Rivermax together with NVIDIA GPU accelerated computing technologies unlocks innovation for a wide range of applications in Media and Entertainment (M&E), Broadcast, Healthcare, Smart Cities and more.    Documentation CenterNVIDIA Virtual Reality Capture and Replay (VCR) SDK01/23/23        The NVIDIA Virtual Reality Capture and Replay (VCR) SDK enables developers and users to accurately capture and replay VR sessions for performance testing, scene troubleshooting, and more.    Documentation CenterNVIDIA TensorRT-Cloud06/02/24        The Triton Inference Server provides an optimized cloud and edge inferencing solution.    ProductNVIDIA DGX Systems04/24/23        System documentation for the DGX AI supercomputers that deliver world-class performance for large generative AI and mainstream AI workloads.    Data Center / CloudDocumentation CenterNVIDIA Capture SDK01/23/23        NVIDIA Capture SDK (formerly GRID SDK) enables developers to easily and efficiently capture, and optionally encode, the display content.    Documentation CenternvCOMP01/23/23        nvCOMP is a high performance GPU enabled data compression library. Includes both open-source and non-OS components. The nvCOMP library provides fast lossless data compression and decompression using a GPU. It features generic compression interfaces to enable developers to use high-performance GPU compressors in their applications.    ProductNVIDIA DGX Platform11/03/23        Built from the ground up for enterprise AI, the NVIDIA DGX platform incorporates the best of NVIDIA software, infrastructure, and expertise in a modern, unified AI development and training solution. Every aspect of the DGX platform is infused with NVIDIA AI expertise, featuring world-class software, record-breaking NVIDIA-accelerated infrastructure in the cloud or on-premises, and direct access to NVIDIA DGXPerts to speed the ROI of AI for every enterprise.    Hardware / SemiconductorArchitecture / Engineering / ConstructionHPC / Scientific ComputingProductNVIDIA Aerial04/12/23NVIDIA Aerial™ is a suite of accelerated computing platforms, software, and services for designing, simulating, and operating wireless networks. Aerial contains hardened RAN software libraries for telcos, cloud service providers (CSPs), and enterprises building commercial 5G networks. Academic and industry researchers can access Aerial on cloud or on-premises setups for advanced wireless and AI/machine learning (ML) research for 6G.    Edge ComputingTelecommunicationsProductNVIDIA Cloud Functions        Serverless API to deploy and manage AI workloads on GPUs at planetary scale.    Documentation CenterNVIDIA PyTorch01/23/23        NVIDIA works with Facebook and the community to accelerate PyTorch on NVIDIA GPUs in the main PyTorch branch, as well as, with ready-to-run containers in NGC.    ProductNVIDIA NVSHMEM08/28/23        NVIDIA NVSHMEM is an NVIDIA based “shared memory” library that provides an easy-to-use CPU-side interface to allocate pinned memory that is symmetrically distributed across a cluster of NVIDIA GPUs.    Data Center / CloudProductNVIDIA Modulus04/28/23        NVIDIA Modulus is a deep learning framework that blends the power of physics and partial differential equations (PDEs) with AI to build more robust models for better analysis.    HPC / Scientific ComputingSimulation / Modeling / DesignDocumentation CenterNVIDIA Bright Cluster Manager 03/08/23        NVIDIA Bright Cluster Manager offers fast deployment and end-to-end management for heterogeneous HPC and AI server clusters at the edge, in the data center and in multi/hybrid-cloud environments. It automates provisioning and administration for clusters ranging in size from a single node to hundreds of thousands, supports CPU-based and NVIDIA GPU-accelerated systems, and orchestration with Kubernetes.    HPC / Scientific ComputingEdge ComputingData Center / CloudProductNVIDIA Networking09/05/23        Accelerated Networks for Modern Workloads: One-third of the 30 million data center servers shipped each year are consumed running the software-defined data center stack.    NetworkingProductNVIDIA MONAI04/19/23        The NVIDIA MONAI framework is the open-source foundation being created by Project MONAI. MONAI is a freely available, community-supported, PyTorch-based framework for deep learning in healthcare imaging. It provides domain-optimized foundational capabilities for developing healthcare imaging training workflows in a native PyTorch paradigm.    Healthcare & Life SciencesComputer Vision / Video AnalyticsProductNVIDIA NGX02/14/23        NVIDIA NGX makes it easy to integrate pre-built, AI-based features into applications with the NGX SDK, NGX Core Runtime and NGX Update Module. The NGX infrastructure updates the AI-based features on all clients that use it.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA DRIVE01/23/23        Learn how to develop for NVIDIA DRIVE®, a scalable computing platform that enables automakers and Tier-1 suppliers to accelerate production of autonomous vehicles.    Documentation CenterNVIDIA RAPIDS 01/23/23        The RAPIDS data science framework is a collection of libraries for running end-to-end data science pipelines completely on the GPU. The interaction is designed to have a familiar look and feel to working in Python, but utilizes optimized NVIDIA CUDA primitives and high-bandwidth GPU memory under the hood.            Load More    Corporate InfoNVIDIA.com HomeAbout NVIDIA\\u200eNVIDIA DeveloperDeveloper HomeBlogResourcesContact UsDeveloper ProgramPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | ContactCopyright © 2024 NVIDIA Corporation\"),\n",
      " Document(metadata={'source': 'https://docs.nvidia.com/cuda', 'title': 'CUDA Toolkit Documentation 12.5', 'language': 'en'}, page_content='CUDA Toolkit Documentation 12.5Release NotesCUDA Features ArchiveEULAInstallation GuidesQuick Start GuideInstallation Guide WindowsInstallation Guide LinuxProgramming GuidesProgramming GuideBest Practices GuideMaxwell Compatibility GuidePascal Compatibility GuideVolta Compatibility GuideTuring Compatibility GuideNVIDIA Ampere GPU Architecture Compatibility GuideHopper Compatibility GuideAda Compatibility GuideMaxwell Tuning GuidePascal Tuning GuideVolta Tuning GuideTuring Tuning GuideNVIDIA Ampere GPU Architecture Tuning GuideHopper Tuning GuideAda Tuning GuidePTX ISAVideo DecoderPTX InteroperabilityInline PTX AssemblyCUDA API ReferencesCUDA Runtime APICUDA Driver APICUDA Math APIcuBLAScuDLA APINVBLASnvJPEGcuFFTCUBCUDA C++ Standard LibrarycuFile API Reference GuidecuRANDcuSPARSENPPnvJitLinknvFatbinNVRTC (Runtime Compilation)ThrustcuSOLVERPTX Compiler API ReferencesPTX Compiler APIsMiscellaneousCUDA Demo SuiteCUDA on WSLCUDA on EFLOWMulti-Instance GPU (MIG)CUDA CompatibilityCUPTIDebugger APIGPUDirect RDMAGPUDirect StoragevGPUToolsNVCCCUDA-GDBCompute SanitizerNsight Eclipse Plugins Installation GuideNsight Eclipse Plugins EditionNsight SystemsNsight ComputeNsight Visual Studio EditionProfilerCUDA Binary UtilitiesWhite PapersFloating Point and IEEE 754Incomplete-LU and Cholesky Preconditioned Iterative MethodsApplication NotesCUDA for TegraCompiler SDKlibNVVM APIlibdevice User’s GuideNVVM IRlanding »CUDA Toolkit Documentation 12.5 Update 1CUDA Toolkit Archive\\r                  -\\r                 \\r                  Send Feedback\\xa0CUDA Toolkit Documentation 12.5 Update 1\\uf0c1Develop, Optimize and Deploy GPU-Accelerated AppsThe NVIDIA® CUDA® Toolkit provides a development environment for creating high performance GPU-accelerated\\rapplications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated\\rembedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.\\rThe toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime\\rlibrary to deploy your application.Using built-in capabilities for distributing computations across multi-GPU configurations, scientists and researchers\\rcan develop applications that scale from single GPU workstations to cloud installations with thousands of GPUs.Release NotesThe Release Notes for the CUDA Toolkit.CUDA Features ArchiveThe list of CUDA features by release.EULAThe CUDA Toolkit End User License Agreement applies to the NVIDIA CUDA Toolkit, the NVIDIA CUDA Samples, the NVIDIA Display Driver, NVIDIA Nsight tools (Visual Studio Edition), and the associated documentation on CUDA APIs, programming model and development tools. If you do not agree with the terms and conditions of the license agreement, then do not download or use the software.Installation Guides\\uf0c1Quick Start GuideThis guide provides the minimal first-steps instructions for installation and verifying CUDA on a standard system.Installation Guide WindowsThis guide discusses how to install and check for correct operation of the CUDA Development Tools on Microsoft Windows systems.Installation Guide LinuxThis guide discusses how to install and check for correct operation of the CUDA Development Tools on GNU/Linux systems.Programming Guides\\uf0c1Programming GuideThis guide provides a detailed discussion of the CUDA programming model and programming interface. It then describes the hardware implementation, and provides guidance on how to achieve maximum performance. The appendices include a list of all CUDA-enabled devices, detailed description of all extensions to the C++ language, listings of supported mathematical functions, C++ features supported in host and device code, details on texture fetching, technical specifications of various devices, and concludes by introducing the low-level driver API.Best Practices GuideThis guide presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures. The intent is to provide guidelines for obtaining the best performance from NVIDIA GPUs using the CUDA Toolkit.Maxwell Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Maxwell Architecture. This document provides guidance to ensure that your software applications are compatible with Maxwell.Pascal Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Pascal Architecture. This document provides guidance to ensure that your software applications are compatible with Pascal.Volta Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Volta Architecture. This document provides guidance to ensure that your software applications are compatible with Volta.Turing Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Turing Architecture. This document provides guidance to ensure that your software applications are compatible with Turing.NVIDIA Ampere GPU Architecture Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on GPUs based on the NVIDIA Ampere GPU Architecture. This document provides guidance to ensure that your software applications are compatible with NVIDIA Ampere GPU architecture.Hopper Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Hopper GPUs. This document provides guidance to ensure that your software applications are compatible with Hopper architecture.Ada Compatibility GuideThis application note is intended to help developers ensure that their NVIDIA CUDA applications will run properly on the Ada GPUs. This document provides guidance to ensure that your software applications are compatible with Ada architecture.Maxwell Tuning GuideMaxwell is NVIDIA’s 4th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Kepler architecture should typically see speedups on the Maxwell architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Maxwell architectural features.Pascal Tuning GuidePascal is NVIDIA’s 5th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Maxwell architecture should typically see speedups on the Pascal architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Pascal architectural features.Volta Tuning GuideVolta is NVIDIA’s 6th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Pascal architecture should typically see speedups on the Volta architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Volta architectural features.Turing Tuning GuideTuring is NVIDIA’s 7th-generation architecture for CUDA compute applications. Applications that follow the best practices for the Pascal architecture should typically see speedups on the Turing architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Turing architectural features.NVIDIA Ampere GPU Architecture Tuning GuideNVIDIA Ampere GPU Architecture is NVIDIA’s 8th-generation architecture for CUDA compute applications. Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the NVIDIA Ampere GPU Architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging NVIDIA Ampere GPU Architecture’s features.Hopper Tuning GuideHopper GPU Architecture is NVIDIA’s 9th-generation architecture for CUDA compute applications. Applications that follow the best practices for the NVIDIA Volta architecture should typically see speedups on the Hopper GPU Architecture without any code changes. This guide summarizes the ways that applications can be fine-tuned to gain additional speedups by leveraging Hopper GPU Architecture’s features.Ada Tuning GuideThe NVIDIA® Ada GPU architecture is NVIDIA’s latest architecture for CUDA® compute applications. The NVIDIA Ada GPU architecture retains and extends the same CUDA programming model provided by previous NVIDIA GPU architectures such as NVIDIA Ampere and Turing, and applications that follow the best practices for those architectures should typically see speedups on the NVIDIA Ada architecture without any code changes. This guide summarizes the ways that an application can be fine-tuned to gain additional speedups by leveraging the NVIDIA Ada GPU architecture’s features.PTX ISAThis guide provides detailed instructions on the use of PTX, a low-level parallel thread execution virtual machine and instruction set architecture (ISA). PTX exposes the GPU as a data-parallel computing device.Video DecoderNVIDIA Video Decoder (NVCUVID) is deprecated. Instead, use the NVIDIA Video Codec SDK (https://developer.nvidia.com/nvidia-video-codec-sdk).PTX InteroperabilityThis document shows how to write PTX that is ABI-compliant and interoperable with other CUDA code.Inline PTX AssemblyThis document shows how to inline PTX (parallel thread execution) assembly language statements into CUDA code. It describes available assembler statement parameters and constraints, and the document also provides a list of some pitfalls that you may encounter.CUDA API References\\uf0c1CUDA Runtime APIFields in structures might appear in order that is different from the order of declaration.CUDA Driver APIFields in structures might appear in order that is different from the order of declaration.CUDA Math APIThe CUDA math API.cuBLASThe cuBLAS library is an implementation of BLAS (Basic Linear Algebra Subprograms) on top of the NVIDIA CUDA runtime. It allows the user to access the computational resources of NVIDIA Graphical Processing Unit (GPU), but does not auto-parallelize across multiple GPUs.cuDLA APIThe cuDLA API.NVBLASThe NVBLAS library is a multi-GPUs accelerated drop-in BLAS (Basic Linear Algebra Subprograms) built on top of the NVIDIA cuBLAS Library.nvJPEGThe nvJPEG Library provides high-performance GPU accelerated JPEG decoding functionality for image formats commonly used in deep learning and hyperscale multimedia applications.cuFFTThe cuFFT library user guide.CUBThe user guide for CUB.CUDA C++ Standard LibraryThe API reference for libcu++, the CUDA C++ standard library.cuFile API Reference GuideThe NVIDIA® GPUDirect® Storage cuFile API Reference Guide provides information about the preliminary version of the cuFile API reference guide that is used in applications and frameworks to leverage GDS technology and describes the intent, context, and operation of those APIs, which are part of the GDS technology.cuRANDThe cuRAND library user guide.cuSPARSEThe cuSPARSE library user guide.NPPNVIDIA NPP is a library of functions for performing CUDA accelerated processing. The initial set of functionality in the library focuses on imaging and video processing and is widely applicable for developers in these areas. NPP will evolve over time to encompass more of the compute heavy tasks in a variety of problem domains. The NPP library is written to maximize flexibility, while maintaining high performance.nvJitLinkThe user guide for the nvJitLink library.nvFatbinThe user guide for the nvFatbin library.NVRTC (Runtime Compilation)NVRTC is a runtime compilation library for CUDA C++. It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX. The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx, and linked with other modules by cuLinkAddData of the CUDA Driver API. This facility can often provide optimizations and performance not possible in a purely offline static compilation.ThrustThe C++ parallel algorithms library.cuSOLVERThe cuSOLVER library user guide.PTX Compiler API References\\uf0c1PTX Compiler APIsThis guide shows how to compile a PTX program into GPU assembly code using APIs provided by the static PTX Compiler library.Miscellaneous\\uf0c1CUDA Demo SuiteThis document describes the demo applications shipped with the CUDA Demo Suite.CUDA on WSLThis guide is intended to help users get started with using NVIDIA CUDA on Windows Subsystem for Linux (WSL 2). The guide covers installation and running CUDA applications and containers in this environment.Multi-Instance GPU (MIG)This edition of the user guide describes the Multi-Instance GPU feature of the NVIDIA® A100 GPU.CUDA CompatibilityThis document describes CUDA Compatibility, including CUDA Enhanced Compatibility and CUDA Forward Compatible Upgrade.CUPTIThe CUPTI-API. The CUDA Profiling Tools Interface (CUPTI) enables the creation of profiling and tracing tools that target CUDA applications.Debugger APIThe CUDA debugger API.GPUDirect RDMAA technology introduced in Kepler-class GPUs and CUDA 5.0, enabling a direct path for communication between the GPU and a third-party peer device on the PCI Express bus when the devices share the same upstream root complex using standard features of PCI Express. This document introduces the technology and describes the steps necessary to enable a GPUDirect RDMA connection to NVIDIA GPUs within the Linux device driver model.GPUDirect StorageThe documentation for GPUDirect Storage.vGPUvGPUs that support CUDA.Tools\\uf0c1NVCCThis is a reference document for nvcc, the CUDA compiler driver. nvcc accepts a range of conventional compiler options, such as for defining macros and include/library paths, and for steering the compilation process.CUDA-GDBThe NVIDIA tool for debugging CUDA applications running on Linux and QNX, providing developers with a mechanism for debugging CUDA applications running on actual hardware. CUDA-GDB is an extension to the x86-64 port of GDB, the GNU Project debugger.Compute SanitizerThe user guide for Compute Sanitizer.Nsight Eclipse Plugins Installation GuideNsight Eclipse Plugins Installation GuideNsight Eclipse Plugins EditionNsight Eclipse Plugins Edition getting started guideNsight SystemsThe documentation for Nsight Systems.Nsight ComputeThe NVIDIA Nsight Compute is the next-generation interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool.Nsight Visual Studio EditionThe documentation for Nsight Visual Studio Edition.ProfilerThis is the guide to the Profiler.CUDA Binary UtilitiesThe application notes for cuobjdump, nvdisasm, and nvprune.White Papers\\uf0c1Floating Point and IEEE 754A number of issues related to floating point accuracy and compliance are a frequent source of confusion on both CPUs and GPUs. The purpose of this white paper is to discuss the most common issues related to NVIDIA GPUs and to supplement the documentation in the CUDA C++ Programming Guide.Incomplete-LU and Cholesky Preconditioned Iterative MethodsIn this white paper we show how to use the cuSPARSE and cuBLAS libraries to achieve a 2x speedup over CPU in the incomplete-LU and Cholesky preconditioned iterative methods. We focus on the Bi-Conjugate Gradient Stabilized and Conjugate Gradient iterative methods, that can be used to solve large sparse nonsymmetric and symmetric positive definite linear systems, respectively. Also, we comment on the parallel sparse triangular solve, which is an essential building block in these algorithms.Application Notes\\uf0c1CUDA for TegraThis application note provides an overview of NVIDIA® Tegra® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the Tegra® integrated GPU (iGPU). It also discusses EGL interoperability.Compiler SDK\\uf0c1libNVVM APIThe libNVVM API.libdevice User’s GuideThe libdevice library is an LLVM bitcode library that implements common functions for GPU kernels.NVVM IRNVVM IR is a compiler IR (intermediate representation) based on the LLVM IR. The NVVM IR is designed to represent GPU compute kernels (for example, CUDA kernels). High-level language front-ends, like the CUDA C compiler front-end, can generate NVVM IR.Privacy Policy\\r|\\rManage My Privacy\\r|\\rDo Not Sell or Share My Data\\r|\\rTerms of Service\\r|\\rAccessibility\\r|\\rCorporate Policies\\r|\\rProduct Security\\r|\\rContact© Copyright 2007-2024, NVIDIA Corporation & affiliates. All rights reserved.\\r      Last updated on Jul 1, 2024.\\r      '),\n",
      " Document(metadata={'source': 'https://docs.nvidia.com/deeplearning', 'title': 'NVIDIA Documentation Hub - NVIDIA Docs', 'description': 'Get started by exploring the latest technical information and product documentation', 'language': 'en'}, page_content=\"NVIDIA Documentation Hub - NVIDIA DocsSubmit SearchNVIDIA DeveloperBlogForumsJoinSubmit SearchNVIDIA DeveloperBlogForumsJoinMenuNVIDIA Documentation HubGet started by exploring the latest technical information and product documentationBrowse byFeaturedProductsAll DocumentsSubmit SearchMost PopularGet support for our latest innovations and see how you can bring them into your own work.NVIDIA API Documentation        Your guide to NVIDIA APIs including NIM and CUDA-X microservices.                Browse            NVIDIA AI Enterprise        NVIDIA AI Enterprise is an end-to-end, cloud-native software platform that accelerates data science pipelines and streamlines development and deployment of production-grade co-pilots and other generative AI applications.                 Browse            NVIDIA Omniverse        NVIDIA Omniverse is a cloud-native, multi-GPU, real-time simulation and collaboration platform for 3D production pipelines based on Pixar's Universal Scene Description (USD) and NVIDIA RTX.                Browse            NVIDIA CUDA        The NVIDIA® CUDA® Toolkit provides a comprehensive development environment for C and C++ developers building GPU-accelerated applications.                Browse            NVIDIA DGX Platform        Built from the ground up for enterprise AI, the NVIDIA DGX platform incorporates the best of NVIDIA software, infrastructure, and expertise in a modern, unified AI development and training solution.                 Browse            NVIDIA cuDNN        The NVIDIA CUDA® Deep Neural Network (cuDNN) is a GPU-accelerated library of primitives for deep neural networks.                 Browse            NVIDIA Jetson        The NVIDIA JetPack SDK, which is the most comprehensive solution for building AI applications, along with L4T and L4T Multimedia, provides the Linux kernel, bootloader, NVIDIA drivers, flashing utilities, sample filesystem, and more for the Jetson platform.                Browse            NVIDIA TensorRT        NVIDIA TensorRT is an SDK for high-performance deep learning inference. It is designed to work in a complementary fashion with training frameworks such as TensorFlow, PyTorch, and MXNet. It focuses specifically on running an already-trained network quickly and efficiently on NVIDIA hardware.                Browse            OpenFiltersClose filtersFiltersSelected FiltersClear AllTopics            AR / VR(4)            Computer Vision / Video Analytics(7)            Content Creation / Rendering(9)            Conversational AI(2)            Cybersecurity(2)            Data Center / Cloud(30)            Data Science(9)            Edge Computing(11)            Generative AI / LLMs(2)            Networking(3)            Recommenders / Personalization(3)            Robotics(4)            Simulation / Modeling / Design(11)See AllSee LessIndustry Segments            Academia / Higher Education(37)            Aerospace(42)            Agriculture(38)            Architecture / Engineering / Construction(41)            Automotive / Transportation(9)            Cloud Services(41)            Consumer Internet(37)            Energy(42)            Financial Services(40)            Gaming(48)            Hardware / Semiconductor(40)            Healthcare & Life Sciences(22)            HPC / Scientific Computing(48)            Manufacturing(40)            Media & Entertainment(44)            Public Sector(42)            Restaurant / Quick-Service(39)            Retail / Consumer Packaged Goods(40)            Telecommunications(39)See AllSee LessJob Roles            Artist / Designer(3)            Business Executive(3)            Data Scientist(23)            Dev / IT Operations(38)            Developer / Engineer(60)            Research: Academic(6)            Research: Non-Academic(6)See AllSee LessAPPLYSort by:Alphabetical - A-ZNewestDocumentation CenterCloudera Data Platform (CDP)04/10/23        The integration of NVIDIA RAPIDS into the Cloudera Data Platform (CDP) provides transparent GPU acceleration of data analytics workloads using Apache Spark. This documentation describes the integration and suggested reference architectures for deployment.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterFleet Command User Guide06/11/24        NVIDIA Fleet Command brings secure edge AI to enterprises of any size. Transform NVIDIA-certified servers into secure edge appliances and connect them to the cloud in minutes. From the cloud, deploy and manage applications from the NGC Catalog or your NGC Private Registry, update system software over-the-air and manage systems remotely with nothing but a browser and internet connection.    Cloud ServicesEdge ComputingData Center / CloudDocumentation CenterGPU Management and Deployment01/23/23        This documentation should be of interest to cluster admins and support personnel of enterprise GPU deployments. It includes monitoring and management tools and application programming interfaces (APIs), in-field diagnostics and health monitoring, and cluster setup and deployment.    Documentation CenterNVIDIA Megatron-Core03/16/24        Developer documentation for Megatron Core covers API documentation, quickstart guide as well as deep dives into advanced GPU  techniques needed to optimize LLM performance at scale.    Documentation CenternvCOMP01/23/23        nvCOMP is a high performance GPU enabled data compression library. Includes both open-source and non-OS components. The nvCOMP library provides fast lossless data compression and decompression using a GPU. It features generic compression interfaces to enable developers to use high-performance GPU compressors in their applications.    ProductNVIDIA Aerial04/12/23NVIDIA Aerial™ is a suite of accelerated computing platforms, software, and services for designing, simulating, and operating wireless networks. Aerial contains hardened RAN software libraries for telcos, cloud service providers (CSPs), and enterprises building commercial 5G networks. Academic and industry researchers can access Aerial on cloud or on-premises setups for advanced wireless and AI/machine learning (ML) research for 6G.    Edge ComputingTelecommunicationsProductNVIDIA AI Enterprise04/27/23        NVIDIA AI Enterprise is an end-to-end, cloud-native software platform that accelerates data science pipelines and streamlines development and deployment of production-grade co-pilots and other generative AI applications. Easy-to-use microservices provide optimized model performance with enterprise-grade security, support, and stability to ensure a smooth transition from prototype to production for enterprises that run their businesses on AI.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA Air06/12/23        A simulation platform that allows users to model data center deployments with full software functionality, creating a digital twin. Transform and streamline network operations by simulating, validating, and automating changes and updates.    Documentation CenterNVIDIA Ansel02/03/23        NVIDIA Ansel is a revolutionary way to capture in-game shots and share the moment. Compose your screenshots from any position, adjust them with post-process filters, capture HDR images in high-fidelity formats, and share them in 360 degrees using your mobile phone, PC, or VR headset.    Documentation CenterNVIDIA API Documentation05/09/24        Your guide to NVIDIA APIs including NIM and CUDA-X microservices.    ProductNVIDIA Base Command Manager10/30/23        NVIDIA Base Command Manager streamlines cluster provisioning, workload management, and infrastructure monitoring. It provides all the tools you need to deploy and manage an AI data center. NVIDIA Base Command Manager Essentials comprises the features of NVIDIA Base Command Manager that are certified for use with NVIDIA AI Enterprise.    Data Center / CloudTechnical OverviewNVIDIA Base Command Platform01/25/23        NVIDIA Base Command Platform is a world-class infrastructure solution for businesses and their data scientists who need a premium AI development experience.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceProductNVIDIA Base OS04/18/23        NVIDIA Base OS implements the stable and fully qualified operating systems for running AI, machine learning, and analytics applications on the DGX platform. It includes system-specific configurations, drivers, and diagnostic and monitoring tools and is available for Ubuntu, Red Hat Enterprise Linux, and Rocky Linux.    Data Center / CloudDocumentation CenterNVIDIA Bright Cluster Manager 03/08/23        NVIDIA Bright Cluster Manager offers fast deployment and end-to-end management for heterogeneous HPC and AI server clusters at the edge, in the data center and in multi/hybrid-cloud environments. It automates provisioning and administration for clusters ranging in size from a single node to hundreds of thousands, supports CPU-based and NVIDIA GPU-accelerated systems, and orchestration with Kubernetes.    HPC / Scientific ComputingEdge ComputingData Center / CloudDocumentation CenterNVIDIA Capture SDK01/23/23        NVIDIA Capture SDK (formerly GRID SDK) enables developers to easily and efficiently capture, and optionally encode, the display content.    Documentation CenterNVIDIA Certification Programs02/06/23        NVIDIA’s program that enables enterprises to confidently deploy hardware solutions that optimally run accelerated workloads—from desktop to data center to edge.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceProductNVIDIA Clara04/27/23        NVIDIA® Clara™ is an open, scalable computing platform that enables developers to build and deploy medical imaging applications into hybrid (embedded, on-premises, or cloud) computing environments to create intelligent instruments and automate healthcare workflows.    Healthcare & Life SciencesComputer Vision / Video AnalyticsProductNVIDIA Cloud Functions        Serverless API to deploy and manage AI workloads on GPUs at planetary scale.    ProductNVIDIA Cloud Native Technologies01/23/23        NVIDIA cloud-native technologies enable developers to build and run GPU-accelerated containers using Docker and Kubernetes.    Cloud ServicesData Center / CloudDocumentation CenterNVIDIA CloudXR SDK02/27/23        CloudXR is NVIDIA's solution for streaming virtual reality (VR), augmented reality (AR), and mixed reality (MR) content from any OpenVR XR application on a remote server--desktop, cloud, data center, or edge.    Documentation CenterNVIDIA Compute Sanitizer04/25/23        Compute Sanitizer is a functional correctness checking suite included in the CUDA toolkit. This suite contains multiple tools that can perform different type of checks. The memcheck tool is capable of precisely detecting and attributing out of bounds and misaligned memory access errors in CUDA applications. The tool can also report hardware exceptions encountered by the GPU. The racecheck tool can report shared memory data access hazards that can cause data races. The initcheck tool can report cases where the GPU performs uninitialized accesses to global memory. The synccheck tool can report cases where the application is attempting invalid usages of synchronization primitives. This document describes the usage of these tools.    ProductNVIDIA CUDA04/03/23        The NVIDIA® CUDA® Toolkit provides a comprehensive development environment for C and C++ developers building GPU-accelerated applications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA cuDNN04/12/23        The NVIDIA CUDA® Deep Neural Network (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, attention, matmul, pooling, and normalization.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceProductNVIDIA cuOpt07/14/23        NVIDIA cuOpt™ is a GPU-accelerated solver that uses heuristics and metaheuristics to solve complex vehicle routing problem variants with a wide range of constraints.    Data ScienceRoboticsProductNVIDIA DALI03/22/23        The NVIDIA Data Loading Library (DALI) is a collection of highly optimized building blocks, and an execution engine, for accelerating the pre-processing of input data for deep learning applications. DALI provides both the performance and the flexibility for accelerating different data pipelines as a single library. This single library can then be easily integrated into different deep learning training and inference applications.    AerospaceHardware / SemiconductorArchitecture / Engineering / ConstructionDocumentation CenterNVIDIA Data Center GPU Drivers01/23/23        NVIDIA Data Center GPU drivers are used in Data Center GPU enterprise deployments for AI, HPC, and accelerated computing workloads. Documentation includes release notes, supported platforms, and cluster setup and deployment.    Documentation CenterNVIDIA Data Center GPU Manager (DCGM)02/03/23        NVIDIA Data Center GPU Manager (DCGM) is a suite of tools for managing and monitoring NVIDIA Data Center GPUs in cluster environments.    AerospaceHardware / SemiconductorArchitecture / Engineering / ConstructionDocumentation CenterNVIDIA Deep Graph Library (DGL)01/23/23        Deep Graph Library (DGL) is a framework-neutral, easy-to-use, and scalable Python library used for implementing and training Graph Neural Networks (GNN). Being framework-neutral, DGL is easily integrated into an existing PyTorch, TensorFlow, or an Apache MXNet workflow.    Documentation CenterNVIDIA Deep Learning Performance07/27/23        GPUs accelerate machine learning operations by performing calculations in parallel. Many operations, especially those representable as matrix multipliers will see good acceleration right out of the box. Even better performance can be achieved by tweaking operation parameters to efficiently use GPU resources. The performance documents present the tips that we think are most widely useful.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceProductNVIDIA DGX Cloud04/26/24        NVIDIA DGX Cloud is an AI platform for enterprise developers, optimized for the demands of generative AI.    ProductNVIDIA DGX Platform11/03/23        Built from the ground up for enterprise AI, the NVIDIA DGX platform incorporates the best of NVIDIA software, infrastructure, and expertise in a modern, unified AI development and training solution. Every aspect of the DGX platform is infused with NVIDIA AI expertise, featuring world-class software, record-breaking NVIDIA-accelerated infrastructure in the cloud or on-premises, and direct access to NVIDIA DGXPerts to speed the ROI of AI for every enterprise.    Hardware / SemiconductorArchitecture / Engineering / ConstructionHPC / Scientific ComputingProductNVIDIA DGX SuperPOD03/17/23        Deployment and management guides for NVIDIA DGX SuperPOD, an AI data center infrastructure platform that enables IT to deliver performance—without compromise—for every user and workload. DGX SuperPOD offers leadership-class accelerated infrastructure and agile, scalable performance for the most challenging AI and high-performance computing (HPC) workloads, with industry-proven results.    Data Center / CloudProductNVIDIA DGX Systems04/24/23        System documentation for the DGX AI supercomputers that deliver world-class performance for large generative AI and mainstream AI workloads.    Data Center / CloudDocumentation CenterNVIDIA DIGITS02/03/23        The NVIDIA Deep Learning GPU Training System (DIGITS) can be used to rapidly train highly accurate deep neural networks (DNNs) for image classification, segmentation, and object-detection tasks. DIGITS simplifies common deep learning tasks such as managing data, designing and training neural networks on multi-GPU systems, monitoring performance in real time with advanced visualizations, and selecting the best-performing model from the results browser for deployment.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA DRIVE01/23/23        Learn how to develop for NVIDIA DRIVE®, a scalable computing platform that enables automakers and Tier-1 suppliers to accelerate production of autonomous vehicles.    Documentation CenterNVIDIA EGX01/23/23        The NVIDIA EGX platform delivers the power of accelerated AI computing to the edge with a cloud-native software stack (EGX stack), a range of validated servers and devices, Helm charts, and partners who offer EGX through their products and services.    ProductNVIDIA Enterprise Support and Services06/26/23        NVIDIA’s accelerated computing, visualization, and networking solutions are expediting the speed of business outcomes. NVIDIA’s experts are here for you at every step in this fast-paced journey. With our expansive support tiers, fast implementations, robust professional services, market-leading education, and high caliber technical certifications, we are here to help you achieve success with all parts of NVIDIA’s accelerated computing, visualization, and networking platform.    Documentation CenterNVIDIA FLARE (Federated Learning Active Runtime Environment)02/03/23        FLARE (Federated Learning Active Runtime Environment) is Nvidia’s open source extensible SDK that allows researchers and data scientists to adapt existing ML/DL workflow to a privacy preserving federated paradigm. FLARE makes it possible to build robust, generalizable AI models without sharing data.    ProductNVIDIA Gameworks01/25/23        Documentation for GameWorks-related products and technologies, including libraries (NVAPI, OpenAutomate), code samples (DirectX, OpenGL), and developer tools (Nsight, NVIDIA System Profiler).    GamingContent Creation / RenderingDocumentation CenterNVIDIA GeForce NOW Developer Platform02/03/23        The GeForce NOW Developer Platform is an SDK and toolset empowering integration of, interaction with, and testing on the NVIDIA cloud gaming service.    Documentation CenterNVIDIA GPUDirect Storage06/14/24        NVIDIA GPUDirect Storage (GDS) enables the fastest data path between GPU memory and storage by avoiding copies to and from system memory, thereby increasing storage input/output (IO) bandwidth and decreasing latency and CPU utilization.    AerospaceHardware / SemiconductorArchitecture / Engineering / ConstructionProductNVIDIA Grace05/30/24        Grace is NVIDIA’s first datacenter CPU. Comprising 72 high-performance Arm v9 cores and featuring the NVIDIA-proprietary Scalable Coherency Fabric (SCF) network-on-chip for incredible core-to-core communication, memory bandwidth and GPU I/O capabilities, Grace provides a high-performance compute foundation in a low-power system-on-chip.    Data Center / CloudDocumentation CenterNVIDIA GVDB Voxels02/03/23        NVIDIA GVDB Voxels is a new framework for simulation, compute and rendering of sparse voxels on the GPU.    Documentation CenterNVIDIA Highlights02/03/23        NVIDIA Highlights enables automatic video capture of key moments, clutch kills, and match-winning plays, ensuring gamers’ best gaming moments are always saved. Once a Highlight is captured, gamers can simply share it directly to Facebook, YouTube, or Weibo right from GeForce Experience’s in-game overlay. Additionally, they can also clip their favorite 15 seconds and share as an animated GIF - all without leaving the game!    ProductNVIDIA Holoscan07/25/23        NVIDIA Holoscan is a hybrid computing platform for medical devices that combines hardware systems for low-latency sensor and network connectivity, optimized libraries for data processing and AI, and core microservices to run surgical video, ultrasound, medical imaging, and other applications anywhere, from embedded to edge to cloud.    Healthcare & Life SciencesDocumentation CenterNVIDIA HPC SDK01/23/23        The NVIDIA HPC SDK is a comprehensive suite of compilers, libraries, and development tools used for developing HPC applications for the NVIDIA platform.    ProductNVIDIA IGX Orin03/23/23        NVIDIA IGX Orin™ is an industrial-grade platform that combines enterprise-level hardware, software, and support. As a single, holistic platform, IGX allows companies to focus on application development and realize the benefits of AI faster.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA IndeX02/03/23        NVIDIA IndeX is a 3D volumetric interactive visualization SDK that allows scientists and researchers to visualize and interact with massive data sets, make real-time modifications, and navigate to the most pertinent parts of the data, all in real-time, to gather better insights faster. IndeX leverages GPU clusters for scalable, real-time, visualization and computing of multi-valued volumetric data together with embedded geometry data.            Load More    OpenFiltersClose filtersFiltersSelected FiltersClear AllProducts            Deep Learning Performance(1)            NVIDIA Air(1)            NVIDIA Virtual GPU (vGPU)(1)Topics            AR / VR(4)            Computer Vision / Video Analytics(7)            Content Creation / Rendering(9)            Conversational AI(2)            Cybersecurity(2)            Data Center / Cloud(30)            Data Science(9)            Edge Computing(11)            Generative AI / LLMs(2)            Networking(3)            Recommenders / Personalization(3)            Robotics(4)            Simulation / Modeling / Design(11)See AllSee LessIndustry Segments            Academia / Higher Education(37)            Aerospace(42)            Agriculture(38)            Architecture / Engineering / Construction(41)            Automotive / Transportation(9)            Cloud Services(41)            Consumer Internet(37)            Energy(42)            Financial Services(40)            Gaming(48)            Hardware / Semiconductor(40)            Healthcare & Life Sciences(22)            HPC / Scientific Computing(48)            Manufacturing(40)            Media & Entertainment(44)            Public Sector(42)            Restaurant / Quick-Service(39)            Retail / Consumer Packaged Goods(40)            Telecommunications(39)See AllSee LessContent Types            Documentation Center(9)            Product(1)            Technical Guide(1)Job Roles            Artist / Designer(3)            Business Executive(3)            Data Scientist(23)            Dev / IT Operations(38)            Developer / Engineer(60)            Research: Academic(6)            Research: Non-Academic(6)See AllSee LessJob Titles            CEO(2)            CTO(5)            Data Analyst(4)            Data Scientist(9)            Developer / Engineer(56)            Film / Video Editor(2)            Graphic Designer / Animator(2)            Industrial Designer / Product Designer(2)            IT Specialist(15)            Solutions Architect(1)            System Administrator(10)See AllSee LessAPPLYProductNVIDIA Virtual GPU (vGPU) Software01/23/23        NVIDIA virtual GPU (vGPU) software is a graphics virtualization platform that extends the power of NVIDIA GPU technology to virtual desktops and apps, offering improved security, productivity, and cost-efficiency.    ProductNVIDIA Clara04/27/23        NVIDIA® Clara™ is an open, scalable computing platform that enables developers to build and deploy medical imaging applications into hybrid (embedded, on-premises, or cloud) computing environments to create intelligent instruments and automate healthcare workflows.    Healthcare & Life SciencesComputer Vision / Video AnalyticsDocumentation CenterNVIDIA Triton Inference Server04/17/23        NVIDIA Triton Inference Server (formerly TensorRT Inference Server) provides a cloud inferencing solution optimized for NVIDIA GPUs. The server provides an inference service via an HTTP or gRPC endpoint, allowing remote clients to request inferencing for any model being managed by the server.    Documentation CenterNVIDIA Maxine02/03/23        NVIDIA Maxine is a GPU-accelerated SDK with state-of-the-art AI features for developers to build virtual collaboration and content creation applications such as video conferencing and live streaming. Maxine’s AI SDKs, such as Video Effects, Audio Effects, and Augmented Reality (AR) are highly optimized and include modular features that can be chained into end-to-end pipelines to deliver the highest performance possible on GPUs, both on PCs and in data centers.    AerospaceHardware / SemiconductorArchitecture / Engineering / ConstructionProductNVIDIA LaunchPad04/26/23        NVIDIA LaunchPad is a free program that provides users short-term access to a large catalog of hands-on labs. Now enterprises and organizations can immediately tap into the necessary hardware and software stacks to experience end-to-end solution workflows in the areas of AI, data science, 3D design collaboration and simulation, and more.    Edge ComputingData Center / CloudDocumentation CenterNVIDIA GeForce NOW Developer Platform02/03/23        The GeForce NOW Developer Platform is an SDK and toolset empowering integration of, interaction with, and testing on the NVIDIA cloud gaming service.    Documentation CenterNVIDIA VRWorks Graphics02/03/23        VRWorks™ is a comprehensive suite of APIs, libraries, and engines that enable application and headset developers to create amazing virtual reality experiences. VRWorks enables a new level of presence by bringing physically realistic visuals, sound, touch interactions, and simulated environments to virtual reality.    Documentation CenterNVIDIA Data Center GPU Manager (DCGM)02/03/23        NVIDIA Data Center GPU Manager (DCGM) is a suite of tools for managing and monitoring NVIDIA Data Center GPUs in cluster environments.    AerospaceHardware / SemiconductorArchitecture / Engineering / ConstructionProductNVIDIA Optimized Frameworks        Deep learning (DL) frameworks offer building blocks for designing, training, and validating deep neural networks through a high-level programming interface. Widely-used DL frameworks, such as PyTorch, TensorFlow, PyTorch Geometric, DGL, and others, rely on GPU-accelerated libraries, such as cuDNN, NCCL, and DALI to deliver high-performance, multi-GPU-accelerated training.    Documentation CenterNVIDIA GVDB Voxels02/03/23        NVIDIA GVDB Voxels is a new framework for simulation, compute and rendering of sparse voxels on the GPU.    ProductNVIDIA AI Enterprise04/27/23        NVIDIA AI Enterprise is an end-to-end, cloud-native software platform that accelerates data science pipelines and streamlines development and deployment of production-grade co-pilots and other generative AI applications. Easy-to-use microservices provide optimized model performance with enterprise-grade security, support, and stability to ensure a smooth transition from prototype to production for enterprises that run their businesses on AI.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA MAGNUM IO06/27/23        NVIDIA MAGNUM IO™ software development kit (SDK) enables developers to remove input/output (IO) bottlenecks in AI, high performance computing (HPC), data science, and visualization applications, reducing the end-to-end time of their workflows. Magnum IO covers all aspects of data movement between CPUs, GPUsns, DPUs, and storage subsystems in virtualized, containerized, and bare-metal environments.    ProductNVIDIA CUDA04/03/23        The NVIDIA® CUDA® Toolkit provides a comprehensive development environment for C and C++ developers building GPU-accelerated applications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceProductNVIDIA Trusted Computing Solutions07/25/23        Protecting sensitive and proprietary information using strong hardware-based security.    Healthcare & Life SciencesFinancial ServicesData Center / CloudDocumentation CenterCloudera Data Platform (CDP)04/10/23        The integration of NVIDIA RAPIDS into the Cloudera Data Platform (CDP) provides transparent GPU acceleration of data analytics workloads using Apache Spark. This documentation describes the integration and suggested reference architectures for deployment.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA Metropolis Microservices for Jetson01/11/24        Metropolis Microservices for Jetson is a platform that simplifies development, deployment and management of Edge AI applications on NVIDIA Jetson. It provides a modular & extensible architecture for developers to distill large complex applications into smaller modular microservice with APIs to integrate into other apps & services.    ProductNVIDIA TAO03/27/23        The NVIDIA TAO Toolkit eliminates the time-consuming process of building and fine-tuning DNNs from scratch for IVA applications.    Public SectorEdge ComputingComputer Vision / Video AnalyticsDocumentation CenterNVIDIA SDK Manager02/03/23        NVIDIA SDK Manager is an all-in-one tool that bundles developer software and provides an end-to-end development environment setup solution for NVIDIA SDKs. Learn about the prerequisite hardware and software to get started with NVIDIA SDK Manager. See the latest features and updates.    Documentation CenterNVIDIA Certification Programs02/06/23        NVIDIA’s program that enables enterprises to confidently deploy hardware solutions that optimally run accelerated workloads—from desktop to data center to edge.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA Ansel02/03/23        NVIDIA Ansel is a revolutionary way to capture in-game shots and share the moment. Compose your screenshots from any position, adjust them with post-process filters, capture HDR images in high-fidelity formats, and share them in 360 degrees using your mobile phone, PC, or VR headset.    ProductNVIDIA Holoscan07/25/23        NVIDIA Holoscan is a hybrid computing platform for medical devices that combines hardware systems for low-latency sensor and network connectivity, optimized libraries for data processing and AI, and core microservices to run surgical video, ultrasound, medical imaging, and other applications anywhere, from embedded to edge to cloud.    Healthcare & Life SciencesProductRAPIDS Accelerator for Apache Spark09/14/23        The RAPIDS Accelerator for Apache Spark leverages GPUs to accelerate processing by combining the power of the RAPIDS cuDF library and the scale of the Spark distributed computing framework. You can run your existing Apache Spark applications on GPUs with no code change by launching Spark with the RAPIDS Accelerator for Apache Spark plugin jar and enabling a single configuration setting.    Data ScienceProductNVIDIA License System02/16/23        NVIDIA® License System is used to serve a pool of floating licenses to NVIDIA licensed products. The NVIDIA License System is configured with licenses obtained from the NVIDIA Licensing Portal.    Data Center / CloudProductNVIDIA NIM05/31/24        Part of NVIDIA AI Enterprise, NVIDIA NIM is a set of easy-to-use microservices for accelerating the deployment of foundation models on any cloud or data center and helps keep your data secure. NIM has production-grade runtimes including on-going security updates. Run your business applications with stable APIs backed by enterprise-grade support.    Documentation CenterNVIDIA FLARE (Federated Learning Active Runtime Environment)02/03/23        FLARE (Federated Learning Active Runtime Environment) is Nvidia’s open source extensible SDK that allows researchers and data scientists to adapt existing ML/DL workflow to a privacy preserving federated paradigm. FLARE makes it possible to build robust, generalizable AI models without sharing data.    ProductNVIDIA Base OS04/18/23        NVIDIA Base OS implements the stable and fully qualified operating systems for running AI, machine learning, and analytics applications on the DGX platform. It includes system-specific configurations, drivers, and diagnostic and monitoring tools and is available for Ubuntu, Red Hat Enterprise Linux, and Rocky Linux.    Data Center / CloudTechnical OverviewNVIDIA Base Command Platform01/25/23        NVIDIA Base Command Platform is a world-class infrastructure solution for businesses and their data scientists who need a premium AI development experience.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA NVTAGS03/22/23        NVIDIA Topology-Aware GPU Selection (NVTAGS) intelligently and automatically assigns GPUs to MPI processes, which reduces overall GPU-to-GPU communication time for Message Passing Interface (MPI) applications.    HPC / Scientific ComputingData Center / CloudDocumentation CenterNVIDIA Rivermax02/03/23        Unique IP-based solution that boosts video and data streaming performance. Rivermax together with NVIDIA GPU accelerated computing technologies unlocks innovation for a wide range of applications in Media and Entertainment (M&E), Broadcast, Healthcare, Smart Cities and more.    Documentation CenterNVIDIA Virtual Reality Capture and Replay (VCR) SDK01/23/23        The NVIDIA Virtual Reality Capture and Replay (VCR) SDK enables developers and users to accurately capture and replay VR sessions for performance testing, scene troubleshooting, and more.    Documentation CenterNVIDIA TensorRT-Cloud06/02/24        The Triton Inference Server provides an optimized cloud and edge inferencing solution.    ProductNVIDIA DGX Systems04/24/23        System documentation for the DGX AI supercomputers that deliver world-class performance for large generative AI and mainstream AI workloads.    Data Center / CloudDocumentation CenterNVIDIA Capture SDK01/23/23        NVIDIA Capture SDK (formerly GRID SDK) enables developers to easily and efficiently capture, and optionally encode, the display content.    Documentation CenternvCOMP01/23/23        nvCOMP is a high performance GPU enabled data compression library. Includes both open-source and non-OS components. The nvCOMP library provides fast lossless data compression and decompression using a GPU. It features generic compression interfaces to enable developers to use high-performance GPU compressors in their applications.    ProductNVIDIA DGX Platform11/03/23        Built from the ground up for enterprise AI, the NVIDIA DGX platform incorporates the best of NVIDIA software, infrastructure, and expertise in a modern, unified AI development and training solution. Every aspect of the DGX platform is infused with NVIDIA AI expertise, featuring world-class software, record-breaking NVIDIA-accelerated infrastructure in the cloud or on-premises, and direct access to NVIDIA DGXPerts to speed the ROI of AI for every enterprise.    Hardware / SemiconductorArchitecture / Engineering / ConstructionHPC / Scientific ComputingProductNVIDIA Aerial04/12/23NVIDIA Aerial™ is a suite of accelerated computing platforms, software, and services for designing, simulating, and operating wireless networks. Aerial contains hardened RAN software libraries for telcos, cloud service providers (CSPs), and enterprises building commercial 5G networks. Academic and industry researchers can access Aerial on cloud or on-premises setups for advanced wireless and AI/machine learning (ML) research for 6G.    Edge ComputingTelecommunicationsProductNVIDIA Cloud Functions        Serverless API to deploy and manage AI workloads on GPUs at planetary scale.    Documentation CenterNVIDIA PyTorch01/23/23        NVIDIA works with Facebook and the community to accelerate PyTorch on NVIDIA GPUs in the main PyTorch branch, as well as, with ready-to-run containers in NGC.    ProductNVIDIA NVSHMEM08/28/23        NVIDIA NVSHMEM is an NVIDIA based “shared memory” library that provides an easy-to-use CPU-side interface to allocate pinned memory that is symmetrically distributed across a cluster of NVIDIA GPUs.    Data Center / CloudProductNVIDIA Modulus04/28/23        NVIDIA Modulus is a deep learning framework that blends the power of physics and partial differential equations (PDEs) with AI to build more robust models for better analysis.    HPC / Scientific ComputingSimulation / Modeling / DesignDocumentation CenterNVIDIA Bright Cluster Manager 03/08/23        NVIDIA Bright Cluster Manager offers fast deployment and end-to-end management for heterogeneous HPC and AI server clusters at the edge, in the data center and in multi/hybrid-cloud environments. It automates provisioning and administration for clusters ranging in size from a single node to hundreds of thousands, supports CPU-based and NVIDIA GPU-accelerated systems, and orchestration with Kubernetes.    HPC / Scientific ComputingEdge ComputingData Center / CloudProductNVIDIA Networking09/05/23        Accelerated Networks for Modern Workloads: One-third of the 30 million data center servers shipped each year are consumed running the software-defined data center stack.    NetworkingProductNVIDIA MONAI04/19/23        The NVIDIA MONAI framework is the open-source foundation being created by Project MONAI. MONAI is a freely available, community-supported, PyTorch-based framework for deep learning in healthcare imaging. It provides domain-optimized foundational capabilities for developing healthcare imaging training workflows in a native PyTorch paradigm.    Healthcare & Life SciencesComputer Vision / Video AnalyticsProductNVIDIA NGX02/14/23        NVIDIA NGX makes it easy to integrate pre-built, AI-based features into applications with the NGX SDK, NGX Core Runtime and NGX Update Module. The NGX infrastructure updates the AI-based features on all clients that use it.    Architecture / Engineering / ConstructionMedia & EntertainmentRestaurant / Quick-ServiceDocumentation CenterNVIDIA DRIVE01/23/23        Learn how to develop for NVIDIA DRIVE®, a scalable computing platform that enables automakers and Tier-1 suppliers to accelerate production of autonomous vehicles.    Documentation CenterNVIDIA RAPIDS 01/23/23        The RAPIDS data science framework is a collection of libraries for running end-to-end data science pipelines completely on the GPU. The interaction is designed to have a familiar look and feel to working in Python, but utilizes optimized NVIDIA CUDA primitives and high-bandwidth GPU memory under the hood.            Load More    Corporate InfoNVIDIA.com HomeAbout NVIDIA\\u200eNVIDIA DeveloperDeveloper HomeBlogResourcesContact UsDeveloper ProgramPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | ContactCopyright © 2024 NVIDIA Corporation\"),\n",
      " Document(metadata={'source': 'https://docs.nvidia.com/gameworkshttps://docs.nvidia.com/cudnn', 'title': 'NVIDIA cuDNN - NVIDIA Docs', 'description': 'The NVIDIA CUDA® Deep Neural Network (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, attention, matmul, pooling, and normalization.', 'language': 'en'}, page_content='NVIDIA cuDNN - NVIDIA DocsSubmit SearchNVIDIA DeveloperBlogForumsJoinSubmit SearchNVIDIA DeveloperBlogForumsJoinMenuNVIDIA cuDNNSubmit SearchSubmit SearchNVIDIA Docs Hub\\xa0\\xa0NVIDIA cuDNNThe NVIDIA CUDA® Deep Neural Network (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, attention, matmul, pooling, and normalization.DocumentationReferencesDownloadsSupportRelease Notes        Current status, software versions, and known issues for NVIDIA cuDNN.                Browse            Installation Guide        This document provides step-by-step instructions on how to install NVIDIA cuDNN.            Browse            Library API        This library is a context-based API that allows for easy multi-threading and (optional) interoperability with CUDA streams. This API lists the data types and API functions per sub-library.                Browse            Developer Guide        This document explains how to use the NVIDIA cuDNN library. While the NVIDIA cuDNN Library API provides per-function API documentation, the Developer Guide gives a more informal end-to-end story about cuDNN’s key capabilities and how to use them.            Browse            Troubleshooting        This document describes the error reporting and API logging utilities for recording the cuDNN API execution and error information. It also helps answer the most commonly asked questions regarding typical use cases.                Browse            Support Matrix        This document lists the supported versions of the OS, NVIDIA CUDA, the CUDA driver, and the hardware for the latest NVIDIA cuDNN release.            Browse            Software License        This document contains specific license terms and conditions for NVIDIA cuDNN. By accepting this agreement, you agree to comply with all the terms and conditions applicable to the specific product(s) included herein.            Browse            Archives        This document provides access to previously released cuDNN documentation versions.            Browse            cuDNN Library        Click on the green buttons that describe your target platform. Only supported platforms will be shown.                Browse            cuDNN Frontend        The cuDNN Frontend (FE) API is a C++ header-only library that wraps the cuDNN C backend API. Both the FE and backend APIs are entry points to the same set of functionality that is commonly referred to as the \"graph API\".                Browse            NVIDIA Developer Program        Join the NVIDIA Developer Program.                Browse            cuDNN Forum        Explore cuDNN forums.                 Browse            cuDNN Developer        Access the latest NVIDIA cuDNN announcements, news, downloads, and training.                 Browse            Technical Blogs        Find more news and tutorials.                 Browse            Corporate InfoNVIDIA.com HomeAbout NVIDIA\\u200eNVIDIA DeveloperDeveloper HomeBlogResourcesContact UsDeveloper ProgramPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | ContactCopyright © 2024 NVIDIA Corporation'),\n",
      " Document(metadata={'source': 'https://docs.nvidia.com/tensorrt', 'title': 'NVIDIA TensorRT - NVIDIA Docs', 'description': 'NVIDIA® TensorRT™ is an SDK for high-performance deep learning inference.  It is designed to work in a complementary fashion with training frameworks such as TensorFlow, PyTorch, and MXNet. It focuses specifically on running an already-trained network quickly and efficiently on NVIDIA hardware. ', 'language': 'en'}, page_content='NVIDIA TensorRT - NVIDIA DocsSubmit SearchNVIDIA DeveloperBlogForumsJoinSubmit SearchNVIDIA DeveloperBlogForumsJoinMenuNVIDIA TensorRTSubmit SearchSubmit SearchNVIDIA Docs Hub\\xa0\\xa0NVIDIA TensorRT        NVIDIA TensorRT    NVIDIA® TensorRT™ is an SDK for high-performance deep learning inference. It is designed to work in a complementary fashion with training frameworks such as TensorFlow, PyTorch, and MXNet. It focuses specifically on running an already-trained network quickly and efficiently on NVIDIA hardware. TensorRT includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for deep learning inference applications. The core of NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA GPUs. TensorRT takes a trained network, which consists of a network definition and a set of trained parameters, and produces a highly optimized runtime engine that performs inference for that network. Refer to the following TensorRT product documentation for more information.Documentation CenterNVIDIA TensorRT Documentation        These documents provide information regarding the current NVIDIA TensorRT  release.    January 23, 2023 10:29 PMCorporate InfoNVIDIA.com HomeAbout NVIDIA\\u200eNVIDIA DeveloperDeveloper HomeBlogResourcesContact UsDeveloper ProgramPrivacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | ContactCopyright © 2024 NVIDIA Corporation')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# List of URLs you want to load (We will crawl the entire site later)\n",
    "urls = [\n",
    "    \"https://docs.nvidia.com\",\n",
    "    \"https://docs.nvidia.com/cuda\",\n",
    "    \"https://docs.nvidia.com/deeplearning\",\n",
    "    \"https://docs.nvidia.com/gameworks\"\n",
    "    \"https://docs.nvidia.com/cudnn\",\n",
    "    \"https://docs.nvidia.com/tensorrt\",\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "data = []\n",
    "\n",
    "# Loop through each URL and load the page content\n",
    "for url in urls:\n",
    "    loader = WebBaseLoader(url)\n",
    "    page = loader.load()\n",
    "    page[0].page_content = page[0].page_content.replace('\\n', '')\n",
    "    data.extend(page)\n",
    "    \n",
    "\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_docs(data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Collection' object has no attribute 'model_fields'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define the variable \"all_splits\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m all_splits \u001b[38;5;241m=\u001b[39m split_docs(data)\n\u001b[0;32m----> 8\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[1;32m      9\u001b[0m     documents\u001b[38;5;241m=\u001b[39mall_splits,\n\u001b[1;32m     10\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag-chroma\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membeddings,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:921\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    920\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[1;32m    922\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    923\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m    924\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    925\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    926\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[1;32m    927\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[1;32m    928\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[1;32m    929\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m    930\u001b[0m     collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    932\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:854\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    831\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    832\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \n\u001b[1;32m    834\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 854\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    855\u001b[0m         collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[1;32m    856\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m    857\u001b[0m         persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[1;32m    858\u001b[0m         client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[1;32m    859\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m    860\u001b[0m         collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[1;32m    861\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    862\u001b[0m     )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:204\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn, create_collection_if_not_exists)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection_metadata \u001b[38;5;241m=\u001b[39m collection_metadata\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_collection_if_not_exists:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__ensure_collection()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mget_collection(name\u001b[38;5;241m=\u001b[39mcollection_name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:211\u001b[0m, in \u001b[0;36mChroma.__ensure_collection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ensure_collection\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Ensure that the collection exists or create it.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mget_or_create_collection(\n\u001b[1;32m    212\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection_name,\n\u001b[1;32m    213\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    214\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection_metadata,\n\u001b[1;32m    215\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/api/client.py:166\u001b[0m, in \u001b[0;36mClient.get_or_create_collection\u001b[0;34m(self, name, configuration, metadata, embedding_function, data_loader)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_or_create_collection\u001b[39m(\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m     data_loader: Optional[DataLoader[Loadable]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    165\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[0;32m--> 166\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server\u001b[38;5;241m.\u001b[39mget_or_create_collection(\n\u001b[1;32m    167\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    168\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    169\u001b[0m         tenant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtenant,\n\u001b[1;32m    170\u001b[0m         database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[1;32m    171\u001b[0m         configuration\u001b[38;5;241m=\u001b[39mconfiguration,\n\u001b[1;32m    172\u001b[0m     )\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Collection(\n\u001b[1;32m    174\u001b[0m         client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server,\n\u001b[1;32m    175\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    176\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function,\n\u001b[1;32m    177\u001b[0m         data_loader\u001b[38;5;241m=\u001b[39mdata_loader,\n\u001b[1;32m    178\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/api/segment.py:221\u001b[0m, in \u001b[0;36mSegmentAPI.get_or_create_collection\u001b[0;34m(self, name, configuration, metadata, tenant, database)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmentAPI.get_or_create_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m, OpenTelemetryGranularity\u001b[38;5;241m.\u001b[39mOPERATION\n\u001b[1;32m    211\u001b[0m )\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[1;32m    220\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CollectionModel:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[1;32m    222\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    223\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    224\u001b[0m         configuration\u001b[38;5;241m=\u001b[39mconfiguration,\n\u001b[1;32m    225\u001b[0m         get_or_create\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    226\u001b[0m         tenant\u001b[38;5;241m=\u001b[39mtenant,\n\u001b[1;32m    227\u001b[0m         database\u001b[38;5;241m=\u001b[39mdatabase,\n\u001b[1;32m    228\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/api/segment.py:176\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[0;34m(self, name, configuration, metadata, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    164\u001b[0m model \u001b[38;5;241m=\u001b[39m CollectionModel(\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[1;32m    166\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m     dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    174\u001b[0m )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# TODO: Let sysdb create the collection directly from the model\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m coll, created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sysdb\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    178\u001b[0m     name\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    179\u001b[0m     configuration\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_configuration(),\n\u001b[1;32m    180\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    181\u001b[0m     dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# This is lazily populated on the first add\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     get_or_create\u001b[38;5;241m=\u001b[39mget_or_create,\n\u001b[1;32m    183\u001b[0m     tenant\u001b[38;5;241m=\u001b[39mtenant,\n\u001b[1;32m    184\u001b[0m     database\u001b[38;5;241m=\u001b[39mdatabase,\n\u001b[1;32m    185\u001b[0m )\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# TODO: wrap sysdb call in try except and log error if it fails\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/db/mixins/sysdb.py:255\u001b[0m, in \u001b[0;36mSqlSysDB.create_collection\u001b[0;34m(self, id, name, configuration, metadata, dimension, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    241\u001b[0m collections \u001b[38;5;241m=\u001b[39m Table(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollections\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    242\u001b[0m databases \u001b[38;5;241m=\u001b[39m Table(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabases\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m insert_collection \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquerybuilder()\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;241m.\u001b[39minto(collections)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;241m.\u001b[39mcolumns(\n\u001b[1;32m    248\u001b[0m         collections\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    249\u001b[0m         collections\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    250\u001b[0m         collections\u001b[38;5;241m.\u001b[39mconfig_json_str,\n\u001b[1;32m    251\u001b[0m         collections\u001b[38;5;241m.\u001b[39mdimension,\n\u001b[1;32m    252\u001b[0m         collections\u001b[38;5;241m.\u001b[39mdatabase_id,\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m--> 255\u001b[0m         ParameterValue(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muuid_to_db(collection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m])),\n\u001b[1;32m    256\u001b[0m         ParameterValue(collection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    257\u001b[0m         ParameterValue(configuration\u001b[38;5;241m.\u001b[39mto_json_str()),\n\u001b[1;32m    258\u001b[0m         ParameterValue(collection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# Get the database id for the database with the given name and tenant\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquerybuilder()\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;241m.\u001b[39mselect(databases\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;241m.\u001b[39mfrom_(databases)\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;241m.\u001b[39mwhere(databases\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m ParameterValue(database))\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;241m.\u001b[39mwhere(databases\u001b[38;5;241m.\u001b[39mtenant_id \u001b[38;5;241m==\u001b[39m ParameterValue(tenant)),\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    266\u001b[0m )\n\u001b[1;32m    267\u001b[0m sql, params \u001b[38;5;241m=\u001b[39m get_sql(insert_collection, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameter_format())\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/types.py:99\u001b[0m, in \u001b[0;36mCollection.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_configuration()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# For the other model attributes we allow the user to access them directly\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_fields:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Collection' object has no attribute 'model_fields'"
     ]
    }
   ],
   "source": [
    "# Add to ChromaDB vector store\n",
    "#from langchain_community.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Define the variable \"all_splits\"\n",
    "all_splits = split_docs(data)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Collection' object has no attribute 'model_fields'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m embedding_function \u001b[38;5;241m=\u001b[39m SentenceTransformerEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# load it into Chroma\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m db \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(docs, embedding_function)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# query it\u001b[39;00m\n\u001b[1;32m     25\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat did the president say about Ketanji Brown Jackson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:921\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    919\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    920\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(\n\u001b[1;32m    922\u001b[0m     texts\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[1;32m    923\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m    924\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[1;32m    925\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    926\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[1;32m    927\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[1;32m    928\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[1;32m    929\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m    930\u001b[0m     collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    932\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:854\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    831\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    832\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \n\u001b[1;32m    834\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 854\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    855\u001b[0m         collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[1;32m    856\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m    857\u001b[0m         persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[1;32m    858\u001b[0m         client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[1;32m    859\u001b[0m         client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m    860\u001b[0m         collection_metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[1;32m    861\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    862\u001b[0m     )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:204\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn, create_collection_if_not_exists)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection_metadata \u001b[38;5;241m=\u001b[39m collection_metadata\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_collection_if_not_exists:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__ensure_collection()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mget_collection(name\u001b[38;5;241m=\u001b[39mcollection_name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_chroma/vectorstores.py:211\u001b[0m, in \u001b[0;36mChroma.__ensure_collection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ensure_collection\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Ensure that the collection exists or create it.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mget_or_create_collection(\n\u001b[1;32m    212\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection_name,\n\u001b[1;32m    213\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    214\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection_metadata,\n\u001b[1;32m    215\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/api/client.py:166\u001b[0m, in \u001b[0;36mClient.get_or_create_collection\u001b[0;34m(self, name, configuration, metadata, embedding_function, data_loader)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_or_create_collection\u001b[39m(\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m     data_loader: Optional[DataLoader[Loadable]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    165\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[0;32m--> 166\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server\u001b[38;5;241m.\u001b[39mget_or_create_collection(\n\u001b[1;32m    167\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    168\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    169\u001b[0m         tenant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtenant,\n\u001b[1;32m    170\u001b[0m         database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[1;32m    171\u001b[0m         configuration\u001b[38;5;241m=\u001b[39mconfiguration,\n\u001b[1;32m    172\u001b[0m     )\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Collection(\n\u001b[1;32m    174\u001b[0m         client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server,\n\u001b[1;32m    175\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    176\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function,\n\u001b[1;32m    177\u001b[0m         data_loader\u001b[38;5;241m=\u001b[39mdata_loader,\n\u001b[1;32m    178\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/api/segment.py:221\u001b[0m, in \u001b[0;36mSegmentAPI.get_or_create_collection\u001b[0;34m(self, name, configuration, metadata, tenant, database)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmentAPI.get_or_create_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m, OpenTelemetryGranularity\u001b[38;5;241m.\u001b[39mOPERATION\n\u001b[1;32m    211\u001b[0m )\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[1;32m    220\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CollectionModel:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[1;32m    222\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    223\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    224\u001b[0m         configuration\u001b[38;5;241m=\u001b[39mconfiguration,\n\u001b[1;32m    225\u001b[0m         get_or_create\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    226\u001b[0m         tenant\u001b[38;5;241m=\u001b[39mtenant,\n\u001b[1;32m    227\u001b[0m         database\u001b[38;5;241m=\u001b[39mdatabase,\n\u001b[1;32m    228\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/api/segment.py:176\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[0;34m(self, name, configuration, metadata, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    164\u001b[0m model \u001b[38;5;241m=\u001b[39m CollectionModel(\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[1;32m    166\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m     dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    174\u001b[0m )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# TODO: Let sysdb create the collection directly from the model\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m coll, created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sysdb\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    178\u001b[0m     name\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    179\u001b[0m     configuration\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_configuration(),\n\u001b[1;32m    180\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    181\u001b[0m     dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# This is lazily populated on the first add\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     get_or_create\u001b[38;5;241m=\u001b[39mget_or_create,\n\u001b[1;32m    183\u001b[0m     tenant\u001b[38;5;241m=\u001b[39mtenant,\n\u001b[1;32m    184\u001b[0m     database\u001b[38;5;241m=\u001b[39mdatabase,\n\u001b[1;32m    185\u001b[0m )\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# TODO: wrap sysdb call in try except and log error if it fails\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/db/mixins/sysdb.py:255\u001b[0m, in \u001b[0;36mSqlSysDB.create_collection\u001b[0;34m(self, id, name, configuration, metadata, dimension, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    241\u001b[0m collections \u001b[38;5;241m=\u001b[39m Table(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollections\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    242\u001b[0m databases \u001b[38;5;241m=\u001b[39m Table(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabases\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    244\u001b[0m insert_collection \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquerybuilder()\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;241m.\u001b[39minto(collections)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;241m.\u001b[39mcolumns(\n\u001b[1;32m    248\u001b[0m         collections\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    249\u001b[0m         collections\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    250\u001b[0m         collections\u001b[38;5;241m.\u001b[39mconfig_json_str,\n\u001b[1;32m    251\u001b[0m         collections\u001b[38;5;241m.\u001b[39mdimension,\n\u001b[1;32m    252\u001b[0m         collections\u001b[38;5;241m.\u001b[39mdatabase_id,\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m--> 255\u001b[0m         ParameterValue(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muuid_to_db(collection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m])),\n\u001b[1;32m    256\u001b[0m         ParameterValue(collection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    257\u001b[0m         ParameterValue(configuration\u001b[38;5;241m.\u001b[39mto_json_str()),\n\u001b[1;32m    258\u001b[0m         ParameterValue(collection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# Get the database id for the database with the given name and tenant\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquerybuilder()\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;241m.\u001b[39mselect(databases\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;241m.\u001b[39mfrom_(databases)\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;241m.\u001b[39mwhere(databases\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m ParameterValue(database))\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;241m.\u001b[39mwhere(databases\u001b[38;5;241m.\u001b[39mtenant_id \u001b[38;5;241m==\u001b[39m ParameterValue(tenant)),\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    266\u001b[0m )\n\u001b[1;32m    267\u001b[0m sql, params \u001b[38;5;241m=\u001b[39m get_sql(insert_collection, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameter_format())\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/types.py:99\u001b[0m, in \u001b[0;36mCollection.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_configuration()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# For the other model attributes we allow the user to access them directly\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_fields:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Collection' object has no attribute 'model_fields'"
     ]
    }
   ],
   "source": [
    "# import\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# load the document and split it into chunks\n",
    "loader = TextLoader(\"state-of-union.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "\n",
    "# query it\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: chromadb\n",
      "Version: 0.4.24\n",
      "Summary: Chroma.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Jeff Huber <jeff@trychroma.com>, Anton Troynikov <anton@trychroma.com>\n",
      "License: \n",
      "Location: /Users/rosecrisp/anaconda3/lib/python3.11/site-packages\n",
      "Requires: bcrypt, build, chroma-hnswlib, fastapi, grpcio, importlib-resources, kubernetes, mmh3, numpy, onnxruntime, opentelemetry-api, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, opentelemetry-sdk, orjson, overrides, posthog, pulsar-client, pydantic, pypika, PyYAML, requests, tenacity, tokenizers, tqdm, typer, typing-extensions, uvicorn\n",
      "Required-by: langchain-chroma\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show chromadb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
